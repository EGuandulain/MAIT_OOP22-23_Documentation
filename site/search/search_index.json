{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Data Science and Visualization Application An Application for Data Science and Visualization made by students of the Master of Automation and IT program 2022-23. To offer a hands-on, visual, user-friendly experience with data, the application draws on a range of fields like data manipulation, regression, classification, and artificial intelligence. Features of the Application Data Selection : Provides the user with the option to upload their own dataset or work with one of the two pre-defined datasets Works on Comma Seperated Value (CSV) files Automatic delimiter detection for the CSV files Data Preview : Provides the user with a preview of the selected dataset in the form of a dataframe Displays statistical information for the selected dataset (along with information about the scientific background for the pre-defined datasets) Provides the user with additional features to remove NaN values from the dataset (if any), and reset the index Data Smoothing, Interpolation, and Outlier Recognition : Allows the user to smoothen, perform interpolation, and perform outlier recognition on the used dataset using various methods Allows the user to tune the parameters regarding each method and analyze the corresponding effects Provides the user with information about each method and the corresponding parameters Allows the user to download filtered datasets after outlier recognition AI Based Classification and Regression : Allows the user to perform classification on classification-type datasets Allows the user to perform regression on time-series type datasets Provides the user with information about each method and the corresponding parameters Provides the user with necessary textual and graphical results Allows the user to save the models after training Allows the user to upload pre-trained models to test the accuracy ML Based Classification and Regression : Allows the user to perform classification on classification-type datasets Allows the user to perform regression on time-series type datasets Provides the user with information about each method and the corresponding parameters Provides the user with necessary textual and graphical results Allows the user to test the trained models by uploading test datasets Created as part of Data Science and Visualization Application . To install the needed modules: pip install -r requirements.txt","title":"Home"},{"location":"#data-science-and-visualization-application","text":"An Application for Data Science and Visualization made by students of the Master of Automation and IT program 2022-23. To offer a hands-on, visual, user-friendly experience with data, the application draws on a range of fields like data manipulation, regression, classification, and artificial intelligence.","title":"Data Science and Visualization Application"},{"location":"#features-of-the-application","text":"Data Selection : Provides the user with the option to upload their own dataset or work with one of the two pre-defined datasets Works on Comma Seperated Value (CSV) files Automatic delimiter detection for the CSV files Data Preview : Provides the user with a preview of the selected dataset in the form of a dataframe Displays statistical information for the selected dataset (along with information about the scientific background for the pre-defined datasets) Provides the user with additional features to remove NaN values from the dataset (if any), and reset the index Data Smoothing, Interpolation, and Outlier Recognition : Allows the user to smoothen, perform interpolation, and perform outlier recognition on the used dataset using various methods Allows the user to tune the parameters regarding each method and analyze the corresponding effects Provides the user with information about each method and the corresponding parameters Allows the user to download filtered datasets after outlier recognition AI Based Classification and Regression : Allows the user to perform classification on classification-type datasets Allows the user to perform regression on time-series type datasets Provides the user with information about each method and the corresponding parameters Provides the user with necessary textual and graphical results Allows the user to save the models after training Allows the user to upload pre-trained models to test the accuracy ML Based Classification and Regression : Allows the user to perform classification on classification-type datasets Allows the user to perform regression on time-series type datasets Provides the user with information about each method and the corresponding parameters Provides the user with necessary textual and graphical results Allows the user to test the trained models by uploading test datasets Created as part of Data Science and Visualization Application .","title":"Features of the Application"},{"location":"#to-install-the-needed-modules","text":"pip install -r requirements.txt","title":"To install the needed modules:"},{"location":"AI_Classification/","text":"Classification Class diagram General Information Classification is a fundamental task in Artificial Intelligence that involves categorizing data into predefined classes or categories based on their attributes or features. In this task, an algorithm is trained using a labeled dataset, where each data point is associated with a predefined label or class. The algorithm learns to recognize the patterns and features in the data and maps them to the appropriate class or label. Once the algorithm is trained, it can then be used to classify new, unseen data points into the appropriate classes. Classification is used in various applications such as image recognition, sentiment analysis, fraud detection, and spam filtering. Common classification algorithms include decision trees, logistic regression, support vector machines, and neural networks. Neural Networks Neural Networks is a powerful classification algorithm in Artificial Intelligence that is inspired by the structure and function of the human brain. It works by simulating a network of interconnected nodes, or neurons, that process and transmit information. During training, the Neural Network algorithm adjusts the strength of the connections between the neurons to learn the underlying patterns in the data. Once the Neural Network is trained, it can be used to classify new, unseen data points into the appropriate classes. Neural Networks are known for their ability to handle complex, high-dimensional data and can often outperform other classification algorithms in terms of accuracy. They are commonly used in applications such as image recognition, natural language processing, and speech recognition. There are many different types of Neural Networks, such as feedforward Neural Networks, recurrent Neural Networks, and convolutional Neural Networks, each of which is suited to different types of classification problems. Random Forest Random Forest is a popular classification algorithm in Artificial Intelligence that is based on the concept of decision trees. It works by building a large number of decision trees, where each tree is trained on a random subset of the data and a random subset of the input features. During training, the Random Forest algorithm combines the predictions of all the decision trees to make the final classification decision. This approach helps to reduce the risk of overfitting, where the model becomes too specialized to the training data and fails to generalize well to new, unseen data. Random Forest is known for its ability to handle high-dimensional data and noisy data, making it a popular choice in many classification applications such as image recognition, medical diagnosis, and customer segmentation. It is also relatively easy to use and provides useful insights into feature importance, allowing practitioners to understand which features are most predictive of the target variable. Description texts generated by ChatGPT","title":"Classification Overview"},{"location":"AI_Classification/#classification","text":"","title":"Classification"},{"location":"AI_Classification/#class-diagram","text":"","title":"Class diagram"},{"location":"AI_Classification/#general-information","text":"Classification is a fundamental task in Artificial Intelligence that involves categorizing data into predefined classes or categories based on their attributes or features. In this task, an algorithm is trained using a labeled dataset, where each data point is associated with a predefined label or class. The algorithm learns to recognize the patterns and features in the data and maps them to the appropriate class or label. Once the algorithm is trained, it can then be used to classify new, unseen data points into the appropriate classes. Classification is used in various applications such as image recognition, sentiment analysis, fraud detection, and spam filtering. Common classification algorithms include decision trees, logistic regression, support vector machines, and neural networks.","title":"General Information"},{"location":"AI_Classification/#neural-networks","text":"Neural Networks is a powerful classification algorithm in Artificial Intelligence that is inspired by the structure and function of the human brain. It works by simulating a network of interconnected nodes, or neurons, that process and transmit information. During training, the Neural Network algorithm adjusts the strength of the connections between the neurons to learn the underlying patterns in the data. Once the Neural Network is trained, it can be used to classify new, unseen data points into the appropriate classes. Neural Networks are known for their ability to handle complex, high-dimensional data and can often outperform other classification algorithms in terms of accuracy. They are commonly used in applications such as image recognition, natural language processing, and speech recognition. There are many different types of Neural Networks, such as feedforward Neural Networks, recurrent Neural Networks, and convolutional Neural Networks, each of which is suited to different types of classification problems.","title":"Neural Networks"},{"location":"AI_Classification/#random-forest","text":"Random Forest is a popular classification algorithm in Artificial Intelligence that is based on the concept of decision trees. It works by building a large number of decision trees, where each tree is trained on a random subset of the data and a random subset of the input features. During training, the Random Forest algorithm combines the predictions of all the decision trees to make the final classification decision. This approach helps to reduce the risk of overfitting, where the model becomes too specialized to the training data and fails to generalize well to new, unseen data. Random Forest is known for its ability to handle high-dimensional data and noisy data, making it a popular choice in many classification applications such as image recognition, medical diagnosis, and customer segmentation. It is also relatively easy to use and provides useful insights into feature importance, allowing practitioners to understand which features are most predictive of the target variable. Description texts generated by ChatGPT","title":"Random Forest"},{"location":"AI_Regression/","text":"Regression Class diagram General Information Regression is a type of machine learning task in Artificial Intelligence that involves predicting a continuous output value based on one or more input variables. In other words, regression is used to model the relationship between the input variables and a continuous target variable. Regression algorithms are trained using labeled datasets, where the input variables are known, and the corresponding target values are also provided. The algorithm learns to recognize the patterns and relationships between the input variables and the target variable, allowing it to make accurate predictions for new, unseen data. Regression is used in a variety of applications, such as predicting house prices, stock prices, or customer lifetime value. Common regression algorithms include linear regression, polynomial regression, and support vector regression. Neural Networks Neural Networks is a powerful regression algorithm in Artificial Intelligence that is inspired by the structure and function of the human brain. It works by simulating a network of interconnected nodes, or neurons, that process and transmit information. During training, the Neural Network algorithm adjusts the strength of the connections between the neurons to learn the underlying patterns in the data. Once the Neural Network is trained, it can be used to make predictions for new, unseen data points. Neural Networks are known for their ability to handle complex, high-dimensional data and can often outperform other regression algorithms in terms of accuracy. They are commonly used in applications such as predicting house prices, stock prices, or customer lifetime value. There are many different types of Neural Networks, such as feedforward Neural Networks, recurrent Neural Networks, and convolutional Neural Networks, each of which is suited to different types of regression problems. Random Forest Random Forest is a popular regression algorithm in Artificial Intelligence that is based on the concept of decision trees. It works by building a large number of decision trees, where each tree is trained on a random subset of the data and a random subset of the input features. During training, the Random Forest algorithm combines the predictions of all the decision trees to make the final regression prediction. This approach helps to reduce the risk of overfitting, where the model becomes too specialized to the training data and fails to generalize well to new, unseen data. Random Forest is known for its ability to handle high-dimensional data and noisy data, making it a popular choice in many regression applications such as predicting housing prices, stock prices, or customer lifetime value. It is also relatively easy to use and provides useful insights into feature importance, allowing practitioners to understand which features are most predictive of the target variable. Description texts generated by ChatGPT","title":"Regression Overview"},{"location":"AI_Regression/#regression","text":"","title":"Regression"},{"location":"AI_Regression/#class-diagram","text":"","title":"Class diagram"},{"location":"AI_Regression/#general-information","text":"Regression is a type of machine learning task in Artificial Intelligence that involves predicting a continuous output value based on one or more input variables. In other words, regression is used to model the relationship between the input variables and a continuous target variable. Regression algorithms are trained using labeled datasets, where the input variables are known, and the corresponding target values are also provided. The algorithm learns to recognize the patterns and relationships between the input variables and the target variable, allowing it to make accurate predictions for new, unseen data. Regression is used in a variety of applications, such as predicting house prices, stock prices, or customer lifetime value. Common regression algorithms include linear regression, polynomial regression, and support vector regression.","title":"General Information"},{"location":"AI_Regression/#neural-networks","text":"Neural Networks is a powerful regression algorithm in Artificial Intelligence that is inspired by the structure and function of the human brain. It works by simulating a network of interconnected nodes, or neurons, that process and transmit information. During training, the Neural Network algorithm adjusts the strength of the connections between the neurons to learn the underlying patterns in the data. Once the Neural Network is trained, it can be used to make predictions for new, unseen data points. Neural Networks are known for their ability to handle complex, high-dimensional data and can often outperform other regression algorithms in terms of accuracy. They are commonly used in applications such as predicting house prices, stock prices, or customer lifetime value. There are many different types of Neural Networks, such as feedforward Neural Networks, recurrent Neural Networks, and convolutional Neural Networks, each of which is suited to different types of regression problems.","title":"Neural Networks"},{"location":"AI_Regression/#random-forest","text":"Random Forest is a popular regression algorithm in Artificial Intelligence that is based on the concept of decision trees. It works by building a large number of decision trees, where each tree is trained on a random subset of the data and a random subset of the input features. During training, the Random Forest algorithm combines the predictions of all the decision trees to make the final regression prediction. This approach helps to reduce the risk of overfitting, where the model becomes too specialized to the training data and fails to generalize well to new, unseen data. Random Forest is known for its ability to handle high-dimensional data and noisy data, making it a popular choice in many regression applications such as predicting housing prices, stock prices, or customer lifetime value. It is also relatively easy to use and provides useful insights into feature importance, allowing practitioners to understand which features are most predictive of the target variable. Description texts generated by ChatGPT","title":"Random Forest"},{"location":"GUI/","text":"Graphical User Interface using Streamlit The Data Science and Visualization Application was developed using the library Streamlit , which is completely open-source and based on python. The entirely open-source, Python-based library Streamlit was used in the creation of the Data Science and Visualization Application. This makes Streamlit very approachable and relatively simple to understand and use. The program is set up as a series of separate pages, each of which serves a distinct function. The program uses a variety of concepts, including AI, machine learning, and data processing. The project repository contains more details on the application and the associated source code. Why Streamlit ? Open source and free to use. Completely based on python , and hence no prior experience with front-end development is needed. Simple, yet strong API with vast number of supportive elements for creating interactive user interfaces . Support for Github-flavored Markdown , LaTeX expressions and HTML tags. Adding widgets ( st.button , st.checkbox , etc.) to gather user inputs is as simple as declaring variables. Functionality to cache and reload data objects in the current user session. Support for various charting libraries ( matplotlib.pyplot , Bokeh , Altair , Plotly , etc.). Easy installation , setup and running of the developed applications. Structure of the Application : Developed as a collection of multiple individual pages , which can exchange information amongst each other. Creation of (parent) classes from all groups describing the parameters required to run the developed class methods. Possible use of Data classes (or Child classes ) for gathering relevant user inputs (datasets, parameters, etc.) and exchanging information amongst the (parent) classes. Each page describes certain functionality within the application and works primarily with a certain group of (parent and data) classes . On each page, Objects of relevant classes are created and initialized with gathered user-inputs, and relevant class methods are called to generate corresponding textual or visual outputs . The computed results are stored back as class variables , and are then displayed on the user interface . Some of the results are also stored in session cache for further use in subsequent application pages. GUI_Class : Primary class used to store basic information about the selected/uploaded dataset. ( Source Code ) Parameters : arg_df : selected dataframe arg_filename : corresponding file name (if present, example \"divorce.csv\" )","title":"GUI"},{"location":"GUI/#graphical-user-interface-using-streamlit","text":"The Data Science and Visualization Application was developed using the library Streamlit , which is completely open-source and based on python. The entirely open-source, Python-based library Streamlit was used in the creation of the Data Science and Visualization Application. This makes Streamlit very approachable and relatively simple to understand and use. The program is set up as a series of separate pages, each of which serves a distinct function. The program uses a variety of concepts, including AI, machine learning, and data processing. The project repository contains more details on the application and the associated source code.","title":"Graphical User Interface using Streamlit"},{"location":"GUI/#why-streamlit","text":"Open source and free to use. Completely based on python , and hence no prior experience with front-end development is needed. Simple, yet strong API with vast number of supportive elements for creating interactive user interfaces . Support for Github-flavored Markdown , LaTeX expressions and HTML tags. Adding widgets ( st.button , st.checkbox , etc.) to gather user inputs is as simple as declaring variables. Functionality to cache and reload data objects in the current user session. Support for various charting libraries ( matplotlib.pyplot , Bokeh , Altair , Plotly , etc.). Easy installation , setup and running of the developed applications.","title":"Why Streamlit ?"},{"location":"GUI/#structure-of-the-application","text":"Developed as a collection of multiple individual pages , which can exchange information amongst each other. Creation of (parent) classes from all groups describing the parameters required to run the developed class methods. Possible use of Data classes (or Child classes ) for gathering relevant user inputs (datasets, parameters, etc.) and exchanging information amongst the (parent) classes. Each page describes certain functionality within the application and works primarily with a certain group of (parent and data) classes . On each page, Objects of relevant classes are created and initialized with gathered user-inputs, and relevant class methods are called to generate corresponding textual or visual outputs . The computed results are stored back as class variables , and are then displayed on the user interface . Some of the results are also stored in session cache for further use in subsequent application pages.","title":"Structure of the Application :"},{"location":"GUI/#gui_class","text":"Primary class used to store basic information about the selected/uploaded dataset. ( Source Code )","title":"GUI_Class :"},{"location":"GUI/#parameters","text":"arg_df : selected dataframe arg_filename : corresponding file name (if present, example \"divorce.csv\" )","title":"Parameters :"},{"location":"Interpolation/","text":"Interpolation The Interpolation class contains methods that perform different types of interpolation on time-series data. The purpose of this class is to fill missing values or gaps in the time series data, making it easier to analyze using four different techniques: forward fill, linear interpolation, cubic interpolation, and spline interpolation. The interpolated data can then be plotted alongside the original data to visualize the results. Class Attributes Name Type Description Range/values Default df pandas.core.frame.DataFrame Input dataframe - - resample_time str Frequency of time series data 1. A string that represents a fixed frequency offset. Some of the available string aliases include: \u2022 'D': daily frequency \u2022 'W': weekly frequency \u2022 'M': month frequency \u2022 'Q': quarter frequency \u2022 'Y': year frequency 2. A string that represents a custom frequency offset. Some of the available offset strings include: \u2022 '2H': every 2 hours \u2022 '3T'/'3MIN': every 3 minutes \u2022 '5S': every 5 seconds \u2022 '1D10H': every day and 10 hours 10MIN date_column str The name of the date column of the input data frame - date order int Order for the Spline Interpolation 1 - 5 2 Class Methods Class Initialization This method initializes the class attributes. df is the DataFrame containing the time-series data. resample_time is the resampling frequency of the data. date_column is the name of the column containing the dates or timestamps. order is the order of the spline interpolation. The method also calls the set_index method to set the DataFrame index. Indexing and Resampling set_index(self) method sets the DataFrame index to the date_column . self.df[self.date_column] = pd.to_datetime(self.df[self.date_column]) self.df = self.df.set_index(self.df[self.date_column]) The DataFrame is then resampled at the specified frequency in the resample_time . self.df = self.df.resample(self.resample_time).mean().reset_index() It also fills any missing values at the beginning and end of the DataFrame with the first and last valid values respectively. for column in self.df.select_dtypes(include=[np.number]): self.df[column].iloc[0] = self.df[column].iloc[self.df[column].first_valid_index()] for column in self.df.select_dtypes(include=[np.number]): self.df[column].iloc[-1] = self.df[column].fillna(method='ffill', limit=len(self.df)).iloc[-1] Forward Fill Method ffill(self) method fills missing values in the DataFrame using forward fill. This is achieved by creating a copy of the original DataFrame and assigning it to self.interpolated_df. Then, the ffill method is called on this new DataFrame, which fills any missing values with the last observed value before the missing value. This means that the missing value is filled with the most recently observed value. self.interpolated_df = self.df.copy() self.interpolated_df = self.interpolated_df.ffill() The result of the ffill interpolation is stored in a new data frame called interpolated_df . Linear Interpolation linear(self) method performs linear interpolation on any missing values in the numerical columns of the Dataframe. This is achieved by creating a copy of the original DataFrame and assigning it to self.interpolated_df . Then, the interpolate method is called on the numerical columns of this new DataFrame with the method = linear and the limit_direction argument set to both . This method fills any missing values with a linearly interpolated value between the closest valid values in the column. num_cols = self.df.select_dtypes(include=[np.number]).columns self.interpolated_df = self.df.copy() self.interpolated_df[num_cols] = self.interpolated_df[num_cols].interpolate(method='linear', limit_direction='both') The result of the linear interpolation is stored in a new data frame called interpolated_df . Cubic Interpolation cubic(self) method performs cubic interpolation on the numerical columns of the resampled Dataframe. It calls the interpolate method with method='cubic' and the limit_direction argument set to both . num_cols = self.df.select_dtypes(include=[np.number]).columns self.interpolated_df = self.df.copy() self.interpolated_df[num_cols] = self.interpolated_df[num_cols].interpolate(method='cubic', limit_direction='both') It then clips the values of the interpolated data. Any negative values in the interpolated_df DataFrame are replaced with 0 and any values that are greater than the maximum value of the working column before interpolation are also replaced with the maximum value using the same approach to remove outliers and keep data in range. self.interpolated_df[num_cols] = self.interpolated_df[num_cols].clip(lower=0) for col in num_cols: max_val_before = self.df[col].max() self.interpolated_df[col] = self.interpolated_df[col].clip(upper=max_val_before) The result of the cubic interpolation is stored in a new data frame called interpolated_df . Spline Interpolation spline(self) method performs interpolation using the spline method on the numerical columns of the data frame. It takes the order parameter as an argument which determines the order of the spline interpolation. num_cols = self.df.select_dtypes(include=[np.number]).columns self.interpolated_df = self.df.copy() self.interpolated_df[num_cols] = self.interpolated_df[num_cols].interpolate(method='spline',order= self.order, limit_direction='both') It then clips the interpolated values to be non-negative and less than or equal to the maximum value before interpolation for each column. self.interpolated_df[num_cols] = self.interpolated_df[num_cols].clip(lower=0) for col in num_cols: max_val_before = self.df[col].max() self.interpolated_df[col] = self.interpolated_df[col].clip(upper=max_val_before) The result of the spline interpolation is stored in a new data frame called interpolated_df Plot Results The plot_results method takes in two arguments: Name Type Description Range/values Default Working_Column str The column/attribute of the input data frame which the user wants to plot List of the name of all the columns of the input data frame Appliances graph_limit int Number of samples of data on the x axis in line plot 1 to max size of rows/data samples in DataFrame 1000 The method plots the interpolated data against the original data using matplotlib. The interpolated data is plotted as a red line, while the original data is plotted as a blue line. The number of data points to be plotted is specified by the graph_limit argument. fig1, ax1 = plt.subplots(figsize=(15, 5)) ax1.plot(self.interpolated_df.index[:graph_limit], self.interpolated_df[Working_Column][:graph_limit], label='Interpolated Data',color='r', alpha=0.8) ax1.plot(self.df.index[:graph_limit], self.df[Working_Column][:graph_limit], label='Original Data',color='b', alpha=0.8) ax1.grid() ax1.legend() ax1.set_xlabel('Samples') ax1.set_ylabel('Value ') ax1.set_title(f\"Line Plot of Data Interpolation for {Working_Column}\") self.lineplot=fig1","title":"Interpolation"},{"location":"Interpolation/#interpolation","text":"The Interpolation class contains methods that perform different types of interpolation on time-series data. The purpose of this class is to fill missing values or gaps in the time series data, making it easier to analyze using four different techniques: forward fill, linear interpolation, cubic interpolation, and spline interpolation. The interpolated data can then be plotted alongside the original data to visualize the results.","title":"Interpolation"},{"location":"Interpolation/#class-attributes","text":"Name Type Description Range/values Default df pandas.core.frame.DataFrame Input dataframe - - resample_time str Frequency of time series data 1. A string that represents a fixed frequency offset. Some of the available string aliases include: \u2022 'D': daily frequency \u2022 'W': weekly frequency \u2022 'M': month frequency \u2022 'Q': quarter frequency \u2022 'Y': year frequency 2. A string that represents a custom frequency offset. Some of the available offset strings include: \u2022 '2H': every 2 hours \u2022 '3T'/'3MIN': every 3 minutes \u2022 '5S': every 5 seconds \u2022 '1D10H': every day and 10 hours 10MIN date_column str The name of the date column of the input data frame - date order int Order for the Spline Interpolation 1 - 5 2","title":"Class Attributes"},{"location":"Interpolation/#class-methods","text":"","title":"Class Methods"},{"location":"Interpolation/#class-initialization","text":"This method initializes the class attributes. df is the DataFrame containing the time-series data. resample_time is the resampling frequency of the data. date_column is the name of the column containing the dates or timestamps. order is the order of the spline interpolation. The method also calls the set_index method to set the DataFrame index.","title":"Class Initialization"},{"location":"Interpolation/#indexing-and-resampling","text":"set_index(self) method sets the DataFrame index to the date_column . self.df[self.date_column] = pd.to_datetime(self.df[self.date_column]) self.df = self.df.set_index(self.df[self.date_column]) The DataFrame is then resampled at the specified frequency in the resample_time . self.df = self.df.resample(self.resample_time).mean().reset_index() It also fills any missing values at the beginning and end of the DataFrame with the first and last valid values respectively. for column in self.df.select_dtypes(include=[np.number]): self.df[column].iloc[0] = self.df[column].iloc[self.df[column].first_valid_index()] for column in self.df.select_dtypes(include=[np.number]): self.df[column].iloc[-1] = self.df[column].fillna(method='ffill', limit=len(self.df)).iloc[-1]","title":"Indexing and Resampling"},{"location":"Interpolation/#forward-fill-method","text":"ffill(self) method fills missing values in the DataFrame using forward fill. This is achieved by creating a copy of the original DataFrame and assigning it to self.interpolated_df. Then, the ffill method is called on this new DataFrame, which fills any missing values with the last observed value before the missing value. This means that the missing value is filled with the most recently observed value. self.interpolated_df = self.df.copy() self.interpolated_df = self.interpolated_df.ffill() The result of the ffill interpolation is stored in a new data frame called interpolated_df .","title":"Forward Fill Method"},{"location":"Interpolation/#linear-interpolation","text":"linear(self) method performs linear interpolation on any missing values in the numerical columns of the Dataframe. This is achieved by creating a copy of the original DataFrame and assigning it to self.interpolated_df . Then, the interpolate method is called on the numerical columns of this new DataFrame with the method = linear and the limit_direction argument set to both . This method fills any missing values with a linearly interpolated value between the closest valid values in the column. num_cols = self.df.select_dtypes(include=[np.number]).columns self.interpolated_df = self.df.copy() self.interpolated_df[num_cols] = self.interpolated_df[num_cols].interpolate(method='linear', limit_direction='both') The result of the linear interpolation is stored in a new data frame called interpolated_df .","title":"Linear Interpolation"},{"location":"Interpolation/#cubic-interpolation","text":"cubic(self) method performs cubic interpolation on the numerical columns of the resampled Dataframe. It calls the interpolate method with method='cubic' and the limit_direction argument set to both . num_cols = self.df.select_dtypes(include=[np.number]).columns self.interpolated_df = self.df.copy() self.interpolated_df[num_cols] = self.interpolated_df[num_cols].interpolate(method='cubic', limit_direction='both') It then clips the values of the interpolated data. Any negative values in the interpolated_df DataFrame are replaced with 0 and any values that are greater than the maximum value of the working column before interpolation are also replaced with the maximum value using the same approach to remove outliers and keep data in range. self.interpolated_df[num_cols] = self.interpolated_df[num_cols].clip(lower=0) for col in num_cols: max_val_before = self.df[col].max() self.interpolated_df[col] = self.interpolated_df[col].clip(upper=max_val_before) The result of the cubic interpolation is stored in a new data frame called interpolated_df .","title":"Cubic Interpolation"},{"location":"Interpolation/#spline-interpolation","text":"spline(self) method performs interpolation using the spline method on the numerical columns of the data frame. It takes the order parameter as an argument which determines the order of the spline interpolation. num_cols = self.df.select_dtypes(include=[np.number]).columns self.interpolated_df = self.df.copy() self.interpolated_df[num_cols] = self.interpolated_df[num_cols].interpolate(method='spline',order= self.order, limit_direction='both') It then clips the interpolated values to be non-negative and less than or equal to the maximum value before interpolation for each column. self.interpolated_df[num_cols] = self.interpolated_df[num_cols].clip(lower=0) for col in num_cols: max_val_before = self.df[col].max() self.interpolated_df[col] = self.interpolated_df[col].clip(upper=max_val_before) The result of the spline interpolation is stored in a new data frame called interpolated_df","title":"Spline Interpolation"},{"location":"Interpolation/#plot-results","text":"The plot_results method takes in two arguments: Name Type Description Range/values Default Working_Column str The column/attribute of the input data frame which the user wants to plot List of the name of all the columns of the input data frame Appliances graph_limit int Number of samples of data on the x axis in line plot 1 to max size of rows/data samples in DataFrame 1000 The method plots the interpolated data against the original data using matplotlib. The interpolated data is plotted as a red line, while the original data is plotted as a blue line. The number of data points to be plotted is specified by the graph_limit argument. fig1, ax1 = plt.subplots(figsize=(15, 5)) ax1.plot(self.interpolated_df.index[:graph_limit], self.interpolated_df[Working_Column][:graph_limit], label='Interpolated Data',color='r', alpha=0.8) ax1.plot(self.df.index[:graph_limit], self.df[Working_Column][:graph_limit], label='Original Data',color='b', alpha=0.8) ax1.grid() ax1.legend() ax1.set_xlabel('Samples') ax1.set_ylabel('Value ') ax1.set_title(f\"Line Plot of Data Interpolation for {Working_Column}\") self.lineplot=fig1","title":"Plot Results"},{"location":"classification/","text":"Classification General Information Classification is a supervised machine learning process of categorizing a given set of input data into classes based on one or more variables. A classification problem can be performed on structured and unstructured data to accurately predict whether the data will fall into predetermined categories. Classification in machine learning can require two or more categories of a given data set. Therefore, it generates a probability score to assign the data into a specific category, such as spam or not spam, yes or no, disease or no disease, red or green, male or female, etc. Applied Algorithms Support Vector Machine (SVM) Support Vector Machine (SVM) is a supervised machine learning algorithm used for both classification and regression. Though we say regression problems as well its best suited for classification. The objective of SVM algorithm is to find a hyperplane in an N-dimensional space that distinctly classifies the data points. The dimension of the hyperplane depends upon the number of features. If the number of input features is two, then the hyperplane is just a line. If the number of input features is three, then the hyperplane becomes a 2-D plane. Parameters kernel - The kernel input is used, when it is difficult to classify the data with a straight line or plane. Inputs: Linear, Poly, Rbf, Sigmoid as a string. gamma - Depending on the chosen Kernel a coefficient can be provided (important for rbf,poly and sigmoid) Inputs: scale, auto as a string, user inputs float value. coef0 - Significant for poly and sigmoid kernels. Possible input: user inputs float value. degree - Degree of the polynomial kernel function (\u2018poly\u2019). Must be non-negative. Ignored by all other kernels. Input: User can enter integer value. max_iter - Hard limit on iterations within solver, or -1 for no limit. Input: User can enter integer value. Logistic Regression A logistic regression model predicts a dependent data variable by analysing the relationship between one or more existing independent variables. For example, a logistic regression could be used to predict whether a political candidate will win or lose an election or whether a high school student will be admitted or not to a particular college. These binary outcomes allow straightforward decisions between two alternatives. Parameters solver - Algorithm to use for the optimization problem. Default is \u2018lbfgs\u2019. To choose a solver, you might want to consider the following aspects: For small datasets, \u2018liblinear\u2019 is a good choice, whereas \u2018sag\u2019 and \u2018saga\u2019 are faster for large ones; For multiclass problems, only \u2018newton-cg\u2019, \u2018sag\u2019, \u2018saga\u2019 and \u2018lbfgs\u2019 handle multinomial loss; \u2018liblinear\u2019 is limited to one-versus-rest schemes. \u2018newton-cholesky\u2019 is a good choice for n_samples >> n_features, especially with one-hot encoded categorical features with rare categories. Note that it is limited to binary classification and the one-versus-rest reduction for multiclass classification. Be aware that the memory usage of this solver has a quadratic dependency on n_features because it explicitly computes the Hessian matrix. Input: String value of the solver penalty - The choice of the algorithm depends on the penalty chosen. Supported penalties by solver: \u2018lbfgs\u2019 - [\u2018l2\u2019, None] \u2018liblinear\u2019 - [\u2018l1\u2019, \u2018l2\u2019] \u2018newton-cg\u2019 - [\u2018l2\u2019, None] \u2018newton-cholesky\u2019 - [\u2018l2\u2019, None] \u2018sag\u2019 - [\u2018l2\u2019, None] \u2018saga\u2019 - [\u2018elasticnet\u2019, \u2018l1\u2019, \u2018l2\u2019, None] The penalty applies different methods of regularisation and helps with overfitting. Input: Penalty method as string. random_state - Shuffles the data (relevant for \u2018sag\u2019, \u2018saga\u2019 or \u2018liblinear\u2019). Use a new random number generator seeded by the given integer. Using an int will produce the same results across different calls. Validation with different int values is suggested. Input: Interger value. max_iter - Maximum number of iterations taken for the solvers to converge Input: Integer value. K-Nearest Neighbour (KNN) The k-nearest neighbours (KNN) algorithm is a data classification method for estimating the likelihood that a data point will become a member of one group or the other, based on what group the data points nearest to it belong to. It uses the Input K to determine how many neighbouring points should be considered. Parameters n_neighbors - Number of nearest neighbours used to decide the class. General Input Parameters data - Data used for the classification. Input: directory for the .csv-file used in the process selected_column - Column user selects for dependent variable y. Input: string variable of the column from - The user can influence which columns he wants to select for the independent variable X. This is the start value for the range the user can select. Input: Integer value to - The user can influence which columns he wants to select for the independent variable X. This is the end value for the range the user can select. Input: Integer value user_testdata - The user can upload data and let the trained model predict the class for that data. Input: directory for the .csv-file used in the process","title":"ML Classification"},{"location":"classification/#classification","text":"","title":"Classification"},{"location":"classification/#general-information","text":"Classification is a supervised machine learning process of categorizing a given set of input data into classes based on one or more variables. A classification problem can be performed on structured and unstructured data to accurately predict whether the data will fall into predetermined categories. Classification in machine learning can require two or more categories of a given data set. Therefore, it generates a probability score to assign the data into a specific category, such as spam or not spam, yes or no, disease or no disease, red or green, male or female, etc.","title":"General Information"},{"location":"classification/#applied-algorithms","text":"","title":"Applied Algorithms"},{"location":"classification/#support-vector-machine-svm","text":"Support Vector Machine (SVM) is a supervised machine learning algorithm used for both classification and regression. Though we say regression problems as well its best suited for classification. The objective of SVM algorithm is to find a hyperplane in an N-dimensional space that distinctly classifies the data points. The dimension of the hyperplane depends upon the number of features. If the number of input features is two, then the hyperplane is just a line. If the number of input features is three, then the hyperplane becomes a 2-D plane.","title":"Support Vector Machine (SVM)"},{"location":"classification/#parameters","text":"kernel - The kernel input is used, when it is difficult to classify the data with a straight line or plane. Inputs: Linear, Poly, Rbf, Sigmoid as a string. gamma - Depending on the chosen Kernel a coefficient can be provided (important for rbf,poly and sigmoid) Inputs: scale, auto as a string, user inputs float value. coef0 - Significant for poly and sigmoid kernels. Possible input: user inputs float value. degree - Degree of the polynomial kernel function (\u2018poly\u2019). Must be non-negative. Ignored by all other kernels. Input: User can enter integer value. max_iter - Hard limit on iterations within solver, or -1 for no limit. Input: User can enter integer value.","title":"Parameters"},{"location":"classification/#logistic-regression","text":"A logistic regression model predicts a dependent data variable by analysing the relationship between one or more existing independent variables. For example, a logistic regression could be used to predict whether a political candidate will win or lose an election or whether a high school student will be admitted or not to a particular college. These binary outcomes allow straightforward decisions between two alternatives.","title":"Logistic Regression"},{"location":"classification/#parameters_1","text":"solver - Algorithm to use for the optimization problem. Default is \u2018lbfgs\u2019. To choose a solver, you might want to consider the following aspects: For small datasets, \u2018liblinear\u2019 is a good choice, whereas \u2018sag\u2019 and \u2018saga\u2019 are faster for large ones; For multiclass problems, only \u2018newton-cg\u2019, \u2018sag\u2019, \u2018saga\u2019 and \u2018lbfgs\u2019 handle multinomial loss; \u2018liblinear\u2019 is limited to one-versus-rest schemes. \u2018newton-cholesky\u2019 is a good choice for n_samples >> n_features, especially with one-hot encoded categorical features with rare categories. Note that it is limited to binary classification and the one-versus-rest reduction for multiclass classification. Be aware that the memory usage of this solver has a quadratic dependency on n_features because it explicitly computes the Hessian matrix. Input: String value of the solver penalty - The choice of the algorithm depends on the penalty chosen. Supported penalties by solver: \u2018lbfgs\u2019 - [\u2018l2\u2019, None] \u2018liblinear\u2019 - [\u2018l1\u2019, \u2018l2\u2019] \u2018newton-cg\u2019 - [\u2018l2\u2019, None] \u2018newton-cholesky\u2019 - [\u2018l2\u2019, None] \u2018sag\u2019 - [\u2018l2\u2019, None] \u2018saga\u2019 - [\u2018elasticnet\u2019, \u2018l1\u2019, \u2018l2\u2019, None] The penalty applies different methods of regularisation and helps with overfitting. Input: Penalty method as string. random_state - Shuffles the data (relevant for \u2018sag\u2019, \u2018saga\u2019 or \u2018liblinear\u2019). Use a new random number generator seeded by the given integer. Using an int will produce the same results across different calls. Validation with different int values is suggested. Input: Interger value. max_iter - Maximum number of iterations taken for the solvers to converge Input: Integer value.","title":"Parameters"},{"location":"classification/#k-nearest-neighbour-knn","text":"The k-nearest neighbours (KNN) algorithm is a data classification method for estimating the likelihood that a data point will become a member of one group or the other, based on what group the data points nearest to it belong to. It uses the Input K to determine how many neighbouring points should be considered.","title":"K-Nearest Neighbour (KNN)"},{"location":"classification/#parameters_2","text":"n_neighbors - Number of nearest neighbours used to decide the class.","title":"Parameters"},{"location":"classification/#general-input-parameters","text":"data - Data used for the classification. Input: directory for the .csv-file used in the process selected_column - Column user selects for dependent variable y. Input: string variable of the column from - The user can influence which columns he wants to select for the independent variable X. This is the start value for the range the user can select. Input: Integer value to - The user can influence which columns he wants to select for the independent variable X. This is the end value for the range the user can select. Input: Integer value user_testdata - The user can upload data and let the trained model predict the class for that data. Input: directory for the .csv-file used in the process","title":"General Input Parameters"},{"location":"outliers/","text":"Outlier Recognition The goal of smoothing is to remove high-frequency noise and highlight important features of the signal, such as trends, patterns, and anomalies. It can be performed using various types of filters and it is used in a wide range of applications like: Time series analysis, Image processing, Signal processing, Data analysis. In each of these applications, smoothing can improve the accuracy of analyses and predictions, and provide a clearer view of important features in the data. Basic Libraries The below lines of the code imports required libraries to perform the outlier recognition such as NumPy, Pandas, SciPy, Matplotlib, and IsolationForest from scikit-learn. NumPy - NumPy is a popular library for numerical computing in Python. Pandas - Pandas is a popular library for data manipulation and analysis in Python. SciPy - The SciPy library is another popular library for scientific computing in Python. Matplotlib - Matplotlib is a popular library for creating visualizations in Python. IsolationForest - Isolation Forest is an unsupervised algorithm used for outlier detection. import numpy as np import pandas as pd import scipy.stats as stats import matplotlib.pyplot as plt from sklearn.ensemble import IsolationForest Class The purpose of this class is to perform outlier detection on the input data. This is class called \"Outliers_Recognization\" that takes two parameters in its constructor method: \"df\" and \"date_column\". These 2 parameters can be used throughout the class. This class can also be further extended to include different types of outlier detection algorithms, allowing for greater flexibility in analysis. class Outliers_Recognization(): def __init__(self, df, date_column): self.df = df self.date_column = date_column df -- is a pandas DataFrame that represents the data we want to analyze for outliers. date_column -- is the name of the column in the Data Frame that contains the date or time data that we want to use for our analysis. Z-Score The Z-Score method is based on the concept of standard deviation. In this method, an outlier is identified as a data point that falls outside a certain number of standard deviations from the mean of the dataset. Paramaters The method is to perform outlier detection on the numerical columns of a Pandas Data Frame using the Z-Score method. The method creates a new Data Frame with the same structure as the original Data Frame, but with any data points that exceed the specified threshold are marked as outliers and replaced with NaN. The method takes two arguments, self and threshold. def Z_score(self, threshold): Self refers to the instance of the class and is passed automatically when the method is called. Threshold is a user-defined value that will be used to determine whether a data point is an outlier or not. This value can be set as per the user. Explanation This line calculates the z-scores of the numerical columns of the DataFrame. First, self.df.select_dtypes(include=[np.number]) selects all columns with numerical data types (e.g. float, int) from the DataFrame. z = np.abs(stats.zscore(self.df[self.df.select_dtypes(include=[np.number]).columns])) Then, stats.zscore from the SciPy library is used to calculate the z-scores of these selected columns. The np.abs function is used to take the absolute value of the z-scores to ensure that all of the scores are positive. This line creates a new DataFrame called self.df_Without_Outliers by masking the original DataFrame with the condition (z > threshold). This means that any data point in the original DataFrame where the z-score is greater than the threshold will be replaced with NaN (i.e. masked). self.df_Without_Outliers = self.df.mask((z > threshold)) This line copies the values from the original DataFrame's date_column to the new self.df_Without_Outliers DataFrame, so that the dates are preserved in the resulting DataFrame. self.df_Without_Outliers[self.date_column] = self.df[self.date_column] Inter Quantile Range The interquartile range (IQR) is a measure of statistical dispersion that is often used in outlier recognition. The IQR is defined as the difference between the first quartile (Q1) and the third quartile (Q3) of a dataset, representing the range of values that contains the middle 50% of the data. This method is known as the Tukey method and is widely used in various fields such as finance, economics, and healthcare. It is important to note that while the IQR can help identify outliers, it should not be the only criterion for outlier recognition Paramaters This function is designed to identify outliers in a given pandas DataFrame using the Quantile method. The function calculates the lower and upper boundaries for each numeric column in the DataFrame using the specified quantile values and then removes any rows containing outliers outside these boundaries. The method takes three arguments, self, Q1 and Q2. def Quantile(self, Q1, Q2): self refers to the instance of the class and is passed automatically when the method is called. Q1 This is a numeric value between 0 and 1 that specifies the lower quantile value for identifying outliers. Any value less than or equal to this quantile will be considered an outlier and removed from the DataFrame. Q2 This is a numeric value between 0 and 1 that specifies the upper quantile value for identifying outliers. Any value greater than or equal to this quantile will be considered an outlier and removed from the DataFrame. Explanation The lower and upper boundaries for each numeric column in the DataFrame is found using the quantile() method of pandas. This step ensures that only numeric columns are considered for outlier detection. L_Q = self.df.select_dtypes(include=[np.number]).quantile(Q1, interpolation=\"nearest\") U_Q = self.df.select_dtypes(include=[np.number]).quantile(Q2, interpolation=\"nearest\") A copy of the original DataFrame without any outliers is created, using the mask() method of pandas and the lower and upper boundaries calculated in the previous step. self.df_Without_Outliers = self.df.mask((self.df.select_dtypes(include=[np.number]) > U_Q) | (self.df.select_dtypes(include=[np.number]) < L_Q)) To assign the original date column values to the new DataFrame without outliers, to preserve the temporal information. self.df_Without_Outliers[self.date_column]=self.df[self.date_column] Modified Z-Score The Modified Z-Score (MZS) is a method for identifying outliers in a dataset based on the median and the median absolute deviation (MAD) of the data. The MZS is a robust alternative to the traditional Z-Score method, which is sensitive to extreme values and outliers. Parameters The function calculates the Modified Z-score for each numeric column in the DataFrame and then masks any rows containing outliers above the specified threshold. The method takes two arguments, self and threshold. def Modified_Z_score(self,threshold): Self refers to the instance of the class and is passed automatically when the method is called. Threshold is a user-defined value that will be used to determine whether a data point is an outlier or not. This value can be set as per the user. Explanation To calculate the median of each numeric column in the DataFrame using the select_dtypes() and median() methods of pandas. This step ensures that only numeric columns are considered for outlier detection. median = self.df.select_dtypes(include=[np.number]).median() This calculates the Median Absolute Deviation (MAD) of each numeric column using the median_abs_deviation() function from the scipy.stats library. MAD = stats.median_abs_deviation(self.df.select_dtypes(include=[np.number])) Calculation of the Modified Z-score for each row and column in the DataFrame using the formula and the calculated median and MAD. z = 0.6745 * np.abs((self.df[self.df.select_dtypes(include=[np.number]).columns] - median) / MAD) The below line of code is used to create a copy of the original DataFrame without any outliers, using the mask() method and any() method of pandas.Modified Z-score greater than or equal to the threshold will be considered an outlier and removed from the DataFrame. self.df_Without_Outliers = self.df.mask((z >= threshold).any(axis=1)) This line copies the values from the original DataFrame's date_column to the new self.df_Without_Outliers DataFrame, so that the dates are preserved in the resulting DataFrame. self.df_Without_Outliers[self.date_column]=self.df[self.date_column] Isolation Forest In the Isolation Forest algorithm, a binary tree is constructed by randomly selecting a feature and a split point for the data at each node. The process is repeated recursively until each data point is isolated in its own leaf node. The path length from the root node to each leaf node is then used as a measure of outlierness, with shorter paths indicating that a data point is more likely to be an outlier. Paramaters The function creates an instance of the IsolationForest class from the scikit-learn library and fits it to the numerical data in the DataFrame. The model then predicts the outliers based on a given contamination rate and removes them from the DataFrame. The method takes two argument, self, contamination. def Isolation_Forest(self,contamination): self refers to the instance of the class and is passed automatically when the method is called. contamination This parameter specifies the proportion of outliers expected in the data. It should be a float value between 0 and 1, where higher values indicate a higher proportion of expected outliers. Explanation The numerical data is extracted from the DataFrame using the select_dtypes() method of pandas and convert it to a numpy array using the values attribute. This is done because the Isolation Forest algorithm can only be applied to numerical data. X = self.df.select_dtypes(include=[np.number]).values An instance of the IsolationForest class with the specified contamination rate is created and fit it to the numerical data using the fit() method. clf = IsolationForest(random_state=0, contamination=contamination) clf.fit(X) The outliers is predicted using the predict() method and identify them as the rows where the predicted value is -1. outliers = clf.predict(X) == -1 A copy of the original DataFrame is created and the values in the numerical columns of the rows identified as outliers are replaced with NaN values, using the loc[] method of pandas. self.df_Without_Outliers = self.df.copy() self.df_Without_Outliers.loc[outliers, self.df.select_dtypes(include=[np.number]).columns] = np.nan The original date column values are assigned to the new DataFrame without outliers, to preserve the temporal information. self.df_Without_Outliers[self.date_column]=self.df[self.date_column]","title":"Outlier Recognition"},{"location":"outliers/#outlier-recognition","text":"The goal of smoothing is to remove high-frequency noise and highlight important features of the signal, such as trends, patterns, and anomalies. It can be performed using various types of filters and it is used in a wide range of applications like: Time series analysis, Image processing, Signal processing, Data analysis. In each of these applications, smoothing can improve the accuracy of analyses and predictions, and provide a clearer view of important features in the data.","title":"Outlier Recognition"},{"location":"outliers/#basic-libraries","text":"The below lines of the code imports required libraries to perform the outlier recognition such as NumPy, Pandas, SciPy, Matplotlib, and IsolationForest from scikit-learn. NumPy - NumPy is a popular library for numerical computing in Python. Pandas - Pandas is a popular library for data manipulation and analysis in Python. SciPy - The SciPy library is another popular library for scientific computing in Python. Matplotlib - Matplotlib is a popular library for creating visualizations in Python. IsolationForest - Isolation Forest is an unsupervised algorithm used for outlier detection. import numpy as np import pandas as pd import scipy.stats as stats import matplotlib.pyplot as plt from sklearn.ensemble import IsolationForest","title":"Basic Libraries"},{"location":"outliers/#class","text":"The purpose of this class is to perform outlier detection on the input data. This is class called \"Outliers_Recognization\" that takes two parameters in its constructor method: \"df\" and \"date_column\". These 2 parameters can be used throughout the class. This class can also be further extended to include different types of outlier detection algorithms, allowing for greater flexibility in analysis. class Outliers_Recognization(): def __init__(self, df, date_column): self.df = df self.date_column = date_column df -- is a pandas DataFrame that represents the data we want to analyze for outliers. date_column -- is the name of the column in the Data Frame that contains the date or time data that we want to use for our analysis.","title":"Class"},{"location":"outliers/#z-score","text":"The Z-Score method is based on the concept of standard deviation. In this method, an outlier is identified as a data point that falls outside a certain number of standard deviations from the mean of the dataset.","title":"Z-Score"},{"location":"outliers/#paramaters","text":"The method is to perform outlier detection on the numerical columns of a Pandas Data Frame using the Z-Score method. The method creates a new Data Frame with the same structure as the original Data Frame, but with any data points that exceed the specified threshold are marked as outliers and replaced with NaN. The method takes two arguments, self and threshold. def Z_score(self, threshold): Self refers to the instance of the class and is passed automatically when the method is called. Threshold is a user-defined value that will be used to determine whether a data point is an outlier or not. This value can be set as per the user.","title":"Paramaters"},{"location":"outliers/#explanation","text":"This line calculates the z-scores of the numerical columns of the DataFrame. First, self.df.select_dtypes(include=[np.number]) selects all columns with numerical data types (e.g. float, int) from the DataFrame. z = np.abs(stats.zscore(self.df[self.df.select_dtypes(include=[np.number]).columns])) Then, stats.zscore from the SciPy library is used to calculate the z-scores of these selected columns. The np.abs function is used to take the absolute value of the z-scores to ensure that all of the scores are positive. This line creates a new DataFrame called self.df_Without_Outliers by masking the original DataFrame with the condition (z > threshold). This means that any data point in the original DataFrame where the z-score is greater than the threshold will be replaced with NaN (i.e. masked). self.df_Without_Outliers = self.df.mask((z > threshold)) This line copies the values from the original DataFrame's date_column to the new self.df_Without_Outliers DataFrame, so that the dates are preserved in the resulting DataFrame. self.df_Without_Outliers[self.date_column] = self.df[self.date_column]","title":"Explanation"},{"location":"outliers/#inter-quantile-range","text":"The interquartile range (IQR) is a measure of statistical dispersion that is often used in outlier recognition. The IQR is defined as the difference between the first quartile (Q1) and the third quartile (Q3) of a dataset, representing the range of values that contains the middle 50% of the data. This method is known as the Tukey method and is widely used in various fields such as finance, economics, and healthcare. It is important to note that while the IQR can help identify outliers, it should not be the only criterion for outlier recognition","title":"Inter Quantile Range"},{"location":"outliers/#paramaters_1","text":"This function is designed to identify outliers in a given pandas DataFrame using the Quantile method. The function calculates the lower and upper boundaries for each numeric column in the DataFrame using the specified quantile values and then removes any rows containing outliers outside these boundaries. The method takes three arguments, self, Q1 and Q2. def Quantile(self, Q1, Q2): self refers to the instance of the class and is passed automatically when the method is called. Q1 This is a numeric value between 0 and 1 that specifies the lower quantile value for identifying outliers. Any value less than or equal to this quantile will be considered an outlier and removed from the DataFrame. Q2 This is a numeric value between 0 and 1 that specifies the upper quantile value for identifying outliers. Any value greater than or equal to this quantile will be considered an outlier and removed from the DataFrame.","title":"Paramaters"},{"location":"outliers/#explanation_1","text":"The lower and upper boundaries for each numeric column in the DataFrame is found using the quantile() method of pandas. This step ensures that only numeric columns are considered for outlier detection. L_Q = self.df.select_dtypes(include=[np.number]).quantile(Q1, interpolation=\"nearest\") U_Q = self.df.select_dtypes(include=[np.number]).quantile(Q2, interpolation=\"nearest\") A copy of the original DataFrame without any outliers is created, using the mask() method of pandas and the lower and upper boundaries calculated in the previous step. self.df_Without_Outliers = self.df.mask((self.df.select_dtypes(include=[np.number]) > U_Q) | (self.df.select_dtypes(include=[np.number]) < L_Q)) To assign the original date column values to the new DataFrame without outliers, to preserve the temporal information. self.df_Without_Outliers[self.date_column]=self.df[self.date_column]","title":"Explanation"},{"location":"outliers/#modified-z-score","text":"The Modified Z-Score (MZS) is a method for identifying outliers in a dataset based on the median and the median absolute deviation (MAD) of the data. The MZS is a robust alternative to the traditional Z-Score method, which is sensitive to extreme values and outliers.","title":"Modified Z-Score"},{"location":"outliers/#parameters","text":"The function calculates the Modified Z-score for each numeric column in the DataFrame and then masks any rows containing outliers above the specified threshold. The method takes two arguments, self and threshold. def Modified_Z_score(self,threshold): Self refers to the instance of the class and is passed automatically when the method is called. Threshold is a user-defined value that will be used to determine whether a data point is an outlier or not. This value can be set as per the user.","title":"Parameters"},{"location":"outliers/#explanation_2","text":"To calculate the median of each numeric column in the DataFrame using the select_dtypes() and median() methods of pandas. This step ensures that only numeric columns are considered for outlier detection. median = self.df.select_dtypes(include=[np.number]).median() This calculates the Median Absolute Deviation (MAD) of each numeric column using the median_abs_deviation() function from the scipy.stats library. MAD = stats.median_abs_deviation(self.df.select_dtypes(include=[np.number])) Calculation of the Modified Z-score for each row and column in the DataFrame using the formula and the calculated median and MAD. z = 0.6745 * np.abs((self.df[self.df.select_dtypes(include=[np.number]).columns] - median) / MAD) The below line of code is used to create a copy of the original DataFrame without any outliers, using the mask() method and any() method of pandas.Modified Z-score greater than or equal to the threshold will be considered an outlier and removed from the DataFrame. self.df_Without_Outliers = self.df.mask((z >= threshold).any(axis=1)) This line copies the values from the original DataFrame's date_column to the new self.df_Without_Outliers DataFrame, so that the dates are preserved in the resulting DataFrame. self.df_Without_Outliers[self.date_column]=self.df[self.date_column]","title":"Explanation"},{"location":"outliers/#isolation-forest","text":"In the Isolation Forest algorithm, a binary tree is constructed by randomly selecting a feature and a split point for the data at each node. The process is repeated recursively until each data point is isolated in its own leaf node. The path length from the root node to each leaf node is then used as a measure of outlierness, with shorter paths indicating that a data point is more likely to be an outlier.","title":"Isolation Forest"},{"location":"outliers/#paramaters_2","text":"The function creates an instance of the IsolationForest class from the scikit-learn library and fits it to the numerical data in the DataFrame. The model then predicts the outliers based on a given contamination rate and removes them from the DataFrame. The method takes two argument, self, contamination. def Isolation_Forest(self,contamination): self refers to the instance of the class and is passed automatically when the method is called. contamination This parameter specifies the proportion of outliers expected in the data. It should be a float value between 0 and 1, where higher values indicate a higher proportion of expected outliers.","title":"Paramaters"},{"location":"outliers/#explanation_3","text":"The numerical data is extracted from the DataFrame using the select_dtypes() method of pandas and convert it to a numpy array using the values attribute. This is done because the Isolation Forest algorithm can only be applied to numerical data. X = self.df.select_dtypes(include=[np.number]).values An instance of the IsolationForest class with the specified contamination rate is created and fit it to the numerical data using the fit() method. clf = IsolationForest(random_state=0, contamination=contamination) clf.fit(X) The outliers is predicted using the predict() method and identify them as the rows where the predicted value is -1. outliers = clf.predict(X) == -1 A copy of the original DataFrame is created and the values in the numerical columns of the rows identified as outliers are replaced with NaN values, using the loc[] method of pandas. self.df_Without_Outliers = self.df.copy() self.df_Without_Outliers.loc[outliers, self.df.select_dtypes(include=[np.number]).columns] = np.nan The original date column values are assigned to the new DataFrame without outliers, to preserve the temporal information. self.df_Without_Outliers[self.date_column]=self.df[self.date_column]","title":"Explanation"},{"location":"smoothing/","text":"Smoothing Data The goal of smoothing is to remove high-frequency noise and highlight important features of the signal, such as trends, patterns, and anomalies. Class attributes Name Type Range/values Description data pandas.core.frame.DataFrame - Dataframe to work with. x_axis String - Select the column that will work as X axis y_axis String - Select the column that will work as Y axis(Only for plotting) x_start String 2 - 15 Initial point of the working range of X. (Only for plotting) x_end String - Final point of the working range of X. (Only for plotting) window_size Int 2 - 15 Refers to the number of data points used to calculate the smoothed value for a given point. grade int - Refers to the order of the polynomial that is fit to the data within the window alpha float 2 - 15 Is the smoothing factor that controls the weight given to past values in the calculation of a smoothed value. The indexes of the start and end of the range are obtained to define the working range: self.index_min = int(self.data.index[ self.data[self.x] == self.x_start].to_list()[0]) self.index_max = int(self.data.index[ self.data[self.x] == self.x_end].to_list()[0]) After that, a new array is created with indexes reset from 0 to N, the algorithms and the plots will be performed on this new array: self.new_x_axis = self.data[self.x][self.index_min: self.index_max] self.new_y_axis = self.data[self.y][self.index_min: self.index_max] self.new_x_axis = self.new_x_axis.reset_index(drop = True) self.new_y_axis = self.new_y_axis.reset_index(drop = True) self.y_filtered = [] self.y_filt_complete = [] Moving Average Filter The moving average filter works by taking the average of a set of data points over a certain window size, and using this average as the estimate of the signal value at any given point. It is used for signal processing, finance, and engineering, where the goal is to remove high-frequency noise and obtain a clearer representation of the underlying signal. Parameters Name Type Range/values Description self.new_y_axis String - Declaration of the Y axis (Data that will be analyzed) self.window_size Int 2 - 15 Refers to the number of data points used to calculate the smoothed value for a given point. Description of the method The method is defined and the array that will display the smoothed values is initialized to 0. def mov_average_filter(self): i = 0 moving_averages = [] A loop will iterate the number of times of the lenght of the data that will be smoothed, minus the size of the window. e.g., 150 rows - 10 as window size = 145 iterations. while i < len(self.new_y_axis) - self.window : On each iteration, starting from position 0, the average of the following N numbers will be calculated and the result will be appended to the new smoothed array. window_sum = self.new_y_axis[i : i + self.window] window_average = round(sum(window_sum) / self.window, 2) moving_averages.append(window_average) i += 1 As a time offset is generated when working with a moving average filter, due to the calculation of the current average based on future/past samples. A compensation before and after the smoothed array is implemented. for i in range (0, round(self.window/2), 1): moving_averages.insert(0,None) for i in range (0, round(self.window/2), 1): moving_averages.append(None) The final smoothed values are returned. y_filtered = moving_averages return y_filtered Savitzky-Golay Filter The Savitzky-Golay smoothing filter works by fitting a polynomial of a certain order to a set of data points and using this polynomial to estimate the value of the signal at any given point.It is useful in situations where it is important to preserve features such as peaks and valleys in the data. Parameters Name Type Range/values Description self.new_y_axis String - Declaration of the Y axis (Data that will be analyzed) self.window_size int 2 - 15 Refers to the number of data points used to calculate the smoothed value for a given point. self.poly_degree int 1 - 5 Refers to the order of the polynomial that is fit to the data within the window Description of the method The method is defined and calculated with the function 'savgol_filter' provided by the scipy.signal library. def savgol_filter(self): y_filtered = savgol_filter(self.new_y_axis, self.window, self.grade) return y_filtered Exponential Smoothing Filter Exponential smoothing works by weighting the past data points in a signal exponentially, with more recent data points receiving higher weight than older data points. The smoothed signal value at any given point is a weighted average of the past data points, with the weights decaying exponentially over time. It is particularly useful when the signal has trends or seasonality, as it can effectively capture these patterns. Parameters Name Type Range/values Description self.new_y_axis String - Declaration of the Y axis (Data that will be analyzed) self.alpha float 0 - 1 Is the smoothing factor that controls the weight given to past values in the calculation of a smoothed value. Description of the method The method is defined and the filtered array is initialized to 0. def exponential_filter(self): self.y_filtered = [self.new_y_axis[0]] A loop iterates from 0 to the lenght of the original data. for i in range(1, len(self.new_y_axis)): On each iteration, the new smoothed value is calculated from the following calculation. The result is appended to the smoothed array. smoothed_val = self.alpha * self.new_y_axis[i] + (1 - self.alpha) * self.y_filtered[i-1] self.y_filtered.append(smoothed_val) Create New DataFrame Generate a new Dataframe with the smoothed values obtained by the method selected Parameters Name Type Range/values Description method_name String - Provide the method selected by the user. df_to_change dataframe - Assign a new dataframe that will be filled with the filtered values. Description of the method After adjusting the parameters acording with the desired output, a for loop is performed to run the filtering algorithm in every column of the dataframe. The result will be stored in the new dataframe called \"df_to_change\" df = self.data column_to_skip = self.x if method_name == 'method_name': for column in df.columns: if column != column_to_skip: #The filtering method will be performed here... df_to_change[column] = filtered_column","title":"Smoothing Data"},{"location":"smoothing/#smoothing-data","text":"The goal of smoothing is to remove high-frequency noise and highlight important features of the signal, such as trends, patterns, and anomalies. Class attributes Name Type Range/values Description data pandas.core.frame.DataFrame - Dataframe to work with. x_axis String - Select the column that will work as X axis y_axis String - Select the column that will work as Y axis(Only for plotting) x_start String 2 - 15 Initial point of the working range of X. (Only for plotting) x_end String - Final point of the working range of X. (Only for plotting) window_size Int 2 - 15 Refers to the number of data points used to calculate the smoothed value for a given point. grade int - Refers to the order of the polynomial that is fit to the data within the window alpha float 2 - 15 Is the smoothing factor that controls the weight given to past values in the calculation of a smoothed value. The indexes of the start and end of the range are obtained to define the working range: self.index_min = int(self.data.index[ self.data[self.x] == self.x_start].to_list()[0]) self.index_max = int(self.data.index[ self.data[self.x] == self.x_end].to_list()[0]) After that, a new array is created with indexes reset from 0 to N, the algorithms and the plots will be performed on this new array: self.new_x_axis = self.data[self.x][self.index_min: self.index_max] self.new_y_axis = self.data[self.y][self.index_min: self.index_max] self.new_x_axis = self.new_x_axis.reset_index(drop = True) self.new_y_axis = self.new_y_axis.reset_index(drop = True) self.y_filtered = [] self.y_filt_complete = []","title":"Smoothing Data"},{"location":"smoothing/#moving-average-filter","text":"The moving average filter works by taking the average of a set of data points over a certain window size, and using this average as the estimate of the signal value at any given point. It is used for signal processing, finance, and engineering, where the goal is to remove high-frequency noise and obtain a clearer representation of the underlying signal.","title":"Moving Average Filter"},{"location":"smoothing/#parameters","text":"Name Type Range/values Description self.new_y_axis String - Declaration of the Y axis (Data that will be analyzed) self.window_size Int 2 - 15 Refers to the number of data points used to calculate the smoothed value for a given point.","title":"Parameters"},{"location":"smoothing/#description-of-the-method","text":"The method is defined and the array that will display the smoothed values is initialized to 0. def mov_average_filter(self): i = 0 moving_averages = [] A loop will iterate the number of times of the lenght of the data that will be smoothed, minus the size of the window. e.g., 150 rows - 10 as window size = 145 iterations. while i < len(self.new_y_axis) - self.window : On each iteration, starting from position 0, the average of the following N numbers will be calculated and the result will be appended to the new smoothed array. window_sum = self.new_y_axis[i : i + self.window] window_average = round(sum(window_sum) / self.window, 2) moving_averages.append(window_average) i += 1 As a time offset is generated when working with a moving average filter, due to the calculation of the current average based on future/past samples. A compensation before and after the smoothed array is implemented. for i in range (0, round(self.window/2), 1): moving_averages.insert(0,None) for i in range (0, round(self.window/2), 1): moving_averages.append(None) The final smoothed values are returned. y_filtered = moving_averages return y_filtered","title":"Description of the method"},{"location":"smoothing/#savitzky-golay-filter","text":"The Savitzky-Golay smoothing filter works by fitting a polynomial of a certain order to a set of data points and using this polynomial to estimate the value of the signal at any given point.It is useful in situations where it is important to preserve features such as peaks and valleys in the data.","title":"Savitzky-Golay Filter"},{"location":"smoothing/#parameters_1","text":"Name Type Range/values Description self.new_y_axis String - Declaration of the Y axis (Data that will be analyzed) self.window_size int 2 - 15 Refers to the number of data points used to calculate the smoothed value for a given point. self.poly_degree int 1 - 5 Refers to the order of the polynomial that is fit to the data within the window","title":"Parameters"},{"location":"smoothing/#description-of-the-method_1","text":"The method is defined and calculated with the function 'savgol_filter' provided by the scipy.signal library. def savgol_filter(self): y_filtered = savgol_filter(self.new_y_axis, self.window, self.grade) return y_filtered","title":"Description of the method"},{"location":"smoothing/#exponential-smoothing-filter","text":"Exponential smoothing works by weighting the past data points in a signal exponentially, with more recent data points receiving higher weight than older data points. The smoothed signal value at any given point is a weighted average of the past data points, with the weights decaying exponentially over time. It is particularly useful when the signal has trends or seasonality, as it can effectively capture these patterns.","title":"Exponential Smoothing Filter"},{"location":"smoothing/#parameters_2","text":"Name Type Range/values Description self.new_y_axis String - Declaration of the Y axis (Data that will be analyzed) self.alpha float 0 - 1 Is the smoothing factor that controls the weight given to past values in the calculation of a smoothed value.","title":"Parameters"},{"location":"smoothing/#description-of-the-method_2","text":"The method is defined and the filtered array is initialized to 0. def exponential_filter(self): self.y_filtered = [self.new_y_axis[0]] A loop iterates from 0 to the lenght of the original data. for i in range(1, len(self.new_y_axis)): On each iteration, the new smoothed value is calculated from the following calculation. The result is appended to the smoothed array. smoothed_val = self.alpha * self.new_y_axis[i] + (1 - self.alpha) * self.y_filtered[i-1] self.y_filtered.append(smoothed_val)","title":"Description of the method"},{"location":"smoothing/#create-new-dataframe","text":"Generate a new Dataframe with the smoothed values obtained by the method selected","title":"Create New DataFrame"},{"location":"smoothing/#parameters_3","text":"Name Type Range/values Description method_name String - Provide the method selected by the user. df_to_change dataframe - Assign a new dataframe that will be filled with the filtered values.","title":"Parameters"},{"location":"smoothing/#description-of-the-method_3","text":"After adjusting the parameters acording with the desired output, a for loop is performed to run the filtering algorithm in every column of the dataframe. The result will be stored in the new dataframe called \"df_to_change\" df = self.data column_to_skip = self.x if method_name == 'method_name': for column in df.columns: if column != column_to_skip: #The filtering method will be performed here... df_to_change[column] = filtered_column","title":"Description of the method"},{"location":"AI_Classification/Classification/","text":"Classification source Classification( data_obj: Classification_Data ) Class for all classification methods Methods: .encode source .encode() Encodes variables that are not integer or float format Returns converted dataframe .split_evidence_labels source .split_evidence_labels( data_obj ) Splits given dataset into evidence and labels Args data_obj : Classification_Data object .plot_confusion_matrix source .plot_confusion_matrix( y_test, predictions, title ) Generates a confusion matrix with given labels and predictions Args y_test : real labels predictions : predicted labels title : Title for the plot Returns matplotlib subplot","title":"Classification.py"},{"location":"AI_Classification/Classification/#_1","text":"","title":""},{"location":"AI_Classification/Classification/#classification","text":"source Classification( data_obj: Classification_Data ) Class for all classification methods Methods:","title":"Classification"},{"location":"AI_Classification/Classification/#encode","text":"source .encode() Encodes variables that are not integer or float format Returns converted dataframe","title":".encode"},{"location":"AI_Classification/Classification/#split_evidence_labels","text":"source .split_evidence_labels( data_obj ) Splits given dataset into evidence and labels Args data_obj : Classification_Data object","title":".split_evidence_labels"},{"location":"AI_Classification/Classification/#plot_confusion_matrix","text":"source .plot_confusion_matrix( y_test, predictions, title ) Generates a confusion matrix with given labels and predictions Args y_test : real labels predictions : predicted labels title : Title for the plot Returns matplotlib subplot","title":".plot_confusion_matrix"},{"location":"AI_Classification/Classification_Data/","text":"Classification_Data source Classification_Data() Parameters Parameter Name Type Range / Values Default Value Used for Description Data Pandas Dataframe - - General Contains the dataset test_size float 0.2-0.8 0.2 General Share/Percentage of Data used for testing, if pretrained model is used, all data (0.99) will be used for testing x_labels list[str] headers from dataframe None General Labels used as evidence for the classification, if None all but y will be used y_label str header from dataframe None General Label of column that contains the classes, if None final column will be used hidden_layers array of ints [32]-[4096, 4096, 4096, 4096, 4096] [64, 64] Neural Net Nodes for each hidden layer, every entry in the array creates a hidden layer with as many nodes as the entry's value training_epochs int 1 - 200 10 Neural Net activation_func string elu, relu, linear, sigmoid, hard_sigmoid, softmax, softplus, tanh, exponential, gelu, selu, softsign, swish \"relu\" Neural Net https://www.tensorflow.org/api_docs/python/tf/keras/activations validation_split Bool True Neural Net Whether during the training a part of the data will already be used for testing after each epoch, needed for accuracy/loss per epoch graphs trees int 1 - 10.000 100 Random Forest Number of trees in the forest model None General Allows user uploaded pre-trained models","title":"Classification_Data.py"},{"location":"AI_Classification/Classification_Data/#_1","text":"","title":""},{"location":"AI_Classification/Classification_Data/#classification_data","text":"source Classification_Data()","title":"Classification_Data"},{"location":"AI_Classification/Classification_Data/#parameters","text":"Parameter Name Type Range / Values Default Value Used for Description Data Pandas Dataframe - - General Contains the dataset test_size float 0.2-0.8 0.2 General Share/Percentage of Data used for testing, if pretrained model is used, all data (0.99) will be used for testing x_labels list[str] headers from dataframe None General Labels used as evidence for the classification, if None all but y will be used y_label str header from dataframe None General Label of column that contains the classes, if None final column will be used hidden_layers array of ints [32]-[4096, 4096, 4096, 4096, 4096] [64, 64] Neural Net Nodes for each hidden layer, every entry in the array creates a hidden layer with as many nodes as the entry's value training_epochs int 1 - 200 10 Neural Net activation_func string elu, relu, linear, sigmoid, hard_sigmoid, softmax, softplus, tanh, exponential, gelu, selu, softsign, swish \"relu\" Neural Net https://www.tensorflow.org/api_docs/python/tf/keras/activations validation_split Bool True Neural Net Whether during the training a part of the data will already be used for testing after each epoch, needed for accuracy/loss per epoch graphs trees int 1 - 10.000 100 Random Forest Number of trees in the forest model None General Allows user uploaded pre-trained models","title":"Parameters"},{"location":"AI_Classification/NN_Classification/","text":"NN_Classification source NN_Classification( data_obj: Classification_Data ) Methods: .get_model source .get_model( data_obj ) Initialize the neural network Args data_obj : Classification_Data object Returns tf.keras.Sequential model (via self.model) .plot source .plot( data_obj ) Creates the plots Args data_obj : Classification_Data object Returns data_object with modified variables","title":"NN_Classification.py"},{"location":"AI_Classification/NN_Classification/#_1","text":"","title":""},{"location":"AI_Classification/NN_Classification/#nn_classification","text":"source NN_Classification( data_obj: Classification_Data ) Methods:","title":"NN_Classification"},{"location":"AI_Classification/NN_Classification/#get_model","text":"source .get_model( data_obj ) Initialize the neural network Args data_obj : Classification_Data object Returns tf.keras.Sequential model (via self.model)","title":".get_model"},{"location":"AI_Classification/NN_Classification/#plot","text":"source .plot( data_obj ) Creates the plots Args data_obj : Classification_Data object Returns data_object with modified variables","title":".plot"},{"location":"AI_Classification/RF_Classification/","text":"RF_Classification source RF_Classification( data_obj: Classification_Data ) Methods: .run_classifier source .run_classifier( data_obj ) Initialize the model, train (if not loaded) and evaluate on test data Args data_obj : Classification_Data object Returns data_obj with modified variables .train_model source .train_model() Initializes and trains the random forest Returns RandomForestClassifier .plot source .plot( data_obj ) Creates the plots Args data_obj : Classification_Data object Returns data_object with modified variables","title":"RF_Classification.py"},{"location":"AI_Classification/RF_Classification/#_1","text":"","title":""},{"location":"AI_Classification/RF_Classification/#rf_classification","text":"source RF_Classification( data_obj: Classification_Data ) Methods:","title":"RF_Classification"},{"location":"AI_Classification/RF_Classification/#run_classifier","text":"source .run_classifier( data_obj ) Initialize the model, train (if not loaded) and evaluate on test data Args data_obj : Classification_Data object Returns data_obj with modified variables","title":".run_classifier"},{"location":"AI_Classification/RF_Classification/#train_model","text":"source .train_model() Initializes and trains the random forest Returns RandomForestClassifier","title":".train_model"},{"location":"AI_Classification/RF_Classification/#plot","text":"source .plot( data_obj ) Creates the plots Args data_obj : Classification_Data object Returns data_object with modified variables","title":".plot"},{"location":"AI_Regression/NN_Regression/","text":"NN_Regression source NN_Regression( data_obj: Regression_Data ) Neural Network Regression. Args data_obj : Regression_Data object Returns data_obj with filled result variables Methods: .run_regressor source .run_regressor( data_obj ) Load or create a model, train model (if applicable), make predictions for trained model or uploaded model if it matches the data. Evaluate and plot results Args data_obj : Regression_Data object Returns data_obj with modified values .train_model source .train_model( data_obj ) Initialize the neural network Args data_obj : Regression_Data object Returns tf.keras.Sequential model (via self.model) .evaluate source .evaluate( data_obj ) Create the evaluation, R2 Score, MAE and MSE Args data_obj : Regression_Data object Returns data_obj with modified values .plot source .plot( data_obj ) Creates the output plots Args data_obj : Regression_Data object Returns data_obj with modified values","title":"NN_Regression.py"},{"location":"AI_Regression/NN_Regression/#_1","text":"","title":""},{"location":"AI_Regression/NN_Regression/#nn_regression","text":"source NN_Regression( data_obj: Regression_Data ) Neural Network Regression. Args data_obj : Regression_Data object Returns data_obj with filled result variables Methods:","title":"NN_Regression"},{"location":"AI_Regression/NN_Regression/#run_regressor","text":"source .run_regressor( data_obj ) Load or create a model, train model (if applicable), make predictions for trained model or uploaded model if it matches the data. Evaluate and plot results Args data_obj : Regression_Data object Returns data_obj with modified values","title":".run_regressor"},{"location":"AI_Regression/NN_Regression/#train_model","text":"source .train_model( data_obj ) Initialize the neural network Args data_obj : Regression_Data object Returns tf.keras.Sequential model (via self.model)","title":".train_model"},{"location":"AI_Regression/NN_Regression/#evaluate","text":"source .evaluate( data_obj ) Create the evaluation, R2 Score, MAE and MSE Args data_obj : Regression_Data object Returns data_obj with modified values","title":".evaluate"},{"location":"AI_Regression/NN_Regression/#plot","text":"source .plot( data_obj ) Creates the output plots Args data_obj : Regression_Data object Returns data_obj with modified values","title":".plot"},{"location":"AI_Regression/RF_Regression/","text":"RF_Regression source RF_Regression( data_obj: Regression_Data ) RandomForest Regression. Args data_obj : Regression_Data object Returns data_obj with filled result variables Methods: .run_regressor source .run_regressor( data_obj ) Load or create a model, train model (if applicable), make predictions for trained model or uploaded model if it matches the data. Evaluate and plot results Args data_obj : Regression_Data object Returns data_obj with modified values .train_model source .train_model() Initialize the random forest Returns sklearn RandomForestRegressor .evaluate source .evaluate( data_obj ) Create the evaluation, R2 Score, MAE and MSE Args data_obj : Regression_Data object Returns data_obj with modified values .plot source .plot( data_obj ) Creates the output plots Args data_obj : Regression_Data object Returns data_obj with modified values","title":"RF_Regression.py"},{"location":"AI_Regression/RF_Regression/#_1","text":"","title":""},{"location":"AI_Regression/RF_Regression/#rf_regression","text":"source RF_Regression( data_obj: Regression_Data ) RandomForest Regression. Args data_obj : Regression_Data object Returns data_obj with filled result variables Methods:","title":"RF_Regression"},{"location":"AI_Regression/RF_Regression/#run_regressor","text":"source .run_regressor( data_obj ) Load or create a model, train model (if applicable), make predictions for trained model or uploaded model if it matches the data. Evaluate and plot results Args data_obj : Regression_Data object Returns data_obj with modified values","title":".run_regressor"},{"location":"AI_Regression/RF_Regression/#train_model","text":"source .train_model() Initialize the random forest Returns sklearn RandomForestRegressor","title":".train_model"},{"location":"AI_Regression/RF_Regression/#evaluate","text":"source .evaluate( data_obj ) Create the evaluation, R2 Score, MAE and MSE Args data_obj : Regression_Data object Returns data_obj with modified values","title":".evaluate"},{"location":"AI_Regression/RF_Regression/#plot","text":"source .plot( data_obj ) Creates the output plots Args data_obj : Regression_Data object Returns data_obj with modified values","title":".plot"},{"location":"AI_Regression/Regression/","text":"Regression source Regression( data_obj: Regression_Data ) Class for all regression methods Methods: .process_data source .process_data( data_obj ) Drop non-numeric columns from the dataframe, tries to find a column with date and converts it to usable format Args data_obj : Regression_Data object Returns modified self.data and data_obj.result_string to inform user of processing .split_data source .split_data( data_obj ) Splits given dataset into evidence and labels, requires labels to be last column of dataframe Args data_obj : Regression_Data object Returns modified data_obj .print_results source .print_results( data_obj ) Adds the results to the result_string for the GUI Args data_obj : Regression_Data object Returns modified data_obj .plot_predictions source .plot_predictions( y_scaler, y_test, predictions, data_obj, train_test ) Plots the predicted and real values against each other. Plots as many values as the user selected Args y_scaler : MinMaxScaler used to scale y-data y_test : Real values for y predictions : Predicted values for y data_obj : Regression_Data object train_test : str \"train\" or \"test\" for plot title and correct output to data_obj Returns modified data_obj","title":"Regression.py"},{"location":"AI_Regression/Regression/#_1","text":"","title":""},{"location":"AI_Regression/Regression/#regression","text":"source Regression( data_obj: Regression_Data ) Class for all regression methods Methods:","title":"Regression"},{"location":"AI_Regression/Regression/#process_data","text":"source .process_data( data_obj ) Drop non-numeric columns from the dataframe, tries to find a column with date and converts it to usable format Args data_obj : Regression_Data object Returns modified self.data and data_obj.result_string to inform user of processing","title":".process_data"},{"location":"AI_Regression/Regression/#split_data","text":"source .split_data( data_obj ) Splits given dataset into evidence and labels, requires labels to be last column of dataframe Args data_obj : Regression_Data object Returns modified data_obj","title":".split_data"},{"location":"AI_Regression/Regression/#print_results","text":"source .print_results( data_obj ) Adds the results to the result_string for the GUI Args data_obj : Regression_Data object Returns modified data_obj","title":".print_results"},{"location":"AI_Regression/Regression/#plot_predictions","text":"source .plot_predictions( y_scaler, y_test, predictions, data_obj, train_test ) Plots the predicted and real values against each other. Plots as many values as the user selected Args y_scaler : MinMaxScaler used to scale y-data y_test : Real values for y predictions : Predicted values for y data_obj : Regression_Data object train_test : str \"train\" or \"test\" for plot title and correct output to data_obj Returns modified data_obj","title":".plot_predictions"},{"location":"AI_Regression/Regression_Data/","text":"Regression_Data source Regression_Data() Parameters Input Name Type Range / Values Default Value Used for Description Data Pandas Dataframe - - General Contains the dataset test_size float 0.2-0.8 0.2 General Share/Percentage of Data used for testing, if pretrained model is used, all data (0.99) will be used for testing x_labels list[str] headers from dataframe None General Labels used as evidence for the classification, if None all but y will be used y_label str header from dataframe None General Label of column that contains the classes, if None final column will be used n_values int 20 - test_size*len(data) 50 General Determines how many values are plotted in output graphs hidden_layers array of ints [32]-[4096, 4096, 4096, 4096, 4096] [64, 64] Neural Net Nodes for each hidden layer, every entry in the array creates a hidden layer with as many nodes as the entry's value training_epochs int 1 - 200 10 Neural Net activation_func string elu, relu, linear, sigmoid, hard_sigmoid, softmax, softplus, tanh, exponential, gelu, selu, softsign, swish \"relu\" Neural Net https://www.tensorflow.org/api_docs/python/tf/keras/activations trees int 1 - 10.000 100 Random Forest Number of trees in the forest model None General Allows user uploaded pre-trained models","title":"Regression_Data.py"},{"location":"AI_Regression/Regression_Data/#_1","text":"","title":""},{"location":"AI_Regression/Regression_Data/#regression_data","text":"source Regression_Data()","title":"Regression_Data"},{"location":"AI_Regression/Regression_Data/#parameters","text":"Input Name Type Range / Values Default Value Used for Description Data Pandas Dataframe - - General Contains the dataset test_size float 0.2-0.8 0.2 General Share/Percentage of Data used for testing, if pretrained model is used, all data (0.99) will be used for testing x_labels list[str] headers from dataframe None General Labels used as evidence for the classification, if None all but y will be used y_label str header from dataframe None General Label of column that contains the classes, if None final column will be used n_values int 20 - test_size*len(data) 50 General Determines how many values are plotted in output graphs hidden_layers array of ints [32]-[4096, 4096, 4096, 4096, 4096] [64, 64] Neural Net Nodes for each hidden layer, every entry in the array creates a hidden layer with as many nodes as the entry's value training_epochs int 1 - 200 10 Neural Net activation_func string elu, relu, linear, sigmoid, hard_sigmoid, softmax, softplus, tanh, exponential, gelu, selu, softsign, swish \"relu\" Neural Net https://www.tensorflow.org/api_docs/python/tf/keras/activations trees int 1 - 10.000 100 Random Forest Number of trees in the forest model None General Allows user uploaded pre-trained models","title":"Parameters"},{"location":"Data%20Manipulation/Interpolation/","text":"Interpolation The Interpolation class contains methods that perform different types of interpolation on time-series data. The purpose of this class is to fill missing values or gaps in the time series data, making it easier to analyze using four different techniques: forward fill, linear interpolation, cubic interpolation, and spline interpolation. The interpolated data can then be plotted alongside the original data to visualize the results. Class Attributes Name Type Description Range/values Default df pandas.core.frame.DataFrame Input dataframe - - resample_time str Frequency of time series data 1. A string that represents a fixed frequency offset. Some of the available string aliases include: \u2022 'D': daily frequency \u2022 'W': weekly frequency \u2022 'M': month frequency \u2022 'Q': quarter frequency \u2022 'Y': year frequency 2. A string that represents a custom frequency offset. Some of the available offset strings include: \u2022 '2H': every 2 hours \u2022 '3T'/'3MIN': every 3 minutes \u2022 '5S': every 5 seconds \u2022 '1D10H': every day and 10 hours 10MIN date_column str The name of the date column of the input data frame - date order int Order for the Spline Interpolation 1 - 5 2 Class Methods Class Initialization This method initializes the class attributes. df is the DataFrame containing the time-series data. resample_time is the resampling frequency of the data. date_column is the name of the column containing the dates or timestamps. order is the order of the spline interpolation. The method also calls the set_index method to set the DataFrame index. Indexing and Resampling set_index(self) method sets the DataFrame index to the date_column . self.df[self.date_column] = pd.to_datetime(self.df[self.date_column]) self.df = self.df.set_index(self.df[self.date_column]) The DataFrame is then resampled at the specified frequency in the resample_time . self.df = self.df.resample(self.resample_time).mean().reset_index() It also fills any missing values at the beginning and end of the DataFrame with the first and last valid values respectively. for column in self.df.select_dtypes(include=[np.number]): self.df[column].iloc[0] = self.df[column].iloc[self.df[column].first_valid_index()] for column in self.df.select_dtypes(include=[np.number]): self.df[column].iloc[-1] = self.df[column].fillna(method='ffill', limit=len(self.df)).iloc[-1] Forward Fill Method ffill(self) method fills missing values in the DataFrame using forward fill. This is achieved by creating a copy of the original DataFrame and assigning it to self.interpolated_df. Then, the ffill method is called on this new DataFrame, which fills any missing values with the last observed value before the missing value. This means that the missing value is filled with the most recently observed value. self.interpolated_df = self.df.copy() self.interpolated_df = self.interpolated_df.ffill() The result of the ffill interpolation is stored in a new data frame called interpolated_df . Linear Interpolation linear(self) method performs linear interpolation on any missing values in the numerical columns of the Dataframe. This is achieved by creating a copy of the original DataFrame and assigning it to self.interpolated_df . Then, the interpolate method is called on the numerical columns of this new DataFrame with the method = linear and the limit_direction argument set to both . This method fills any missing values with a linearly interpolated value between the closest valid values in the column. num_cols = self.df.select_dtypes(include=[np.number]).columns self.interpolated_df = self.df.copy() self.interpolated_df[num_cols] = self.interpolated_df[num_cols].interpolate(method='linear', limit_direction='both') The result of the linear interpolation is stored in a new data frame called interpolated_df . Cubic Interpolation cubic(self) method performs cubic interpolation on the numerical columns of the resampled Dataframe. It calls the interpolate method with method='cubic' and the limit_direction argument set to both . num_cols = self.df.select_dtypes(include=[np.number]).columns self.interpolated_df = self.df.copy() self.interpolated_df[num_cols] = self.interpolated_df[num_cols].interpolate(method='cubic', limit_direction='both') It then clips the values of the interpolated data. Any negative values in the interpolated_df DataFrame are replaced with 0 and any values that are greater than the maximum value of the working column before interpolation are also replaced with the maximum value using the same approach to remove outliers and keep data in range. self.interpolated_df[num_cols] = self.interpolated_df[num_cols].clip(lower=0) for col in num_cols: max_val_before = self.df[col].max() self.interpolated_df[col] = self.interpolated_df[col].clip(upper=max_val_before) The result of the cubic interpolation is stored in a new data frame called interpolated_df . Spline Interpolation spline(self) method performs interpolation using the spline method on the numerical columns of the data frame. It takes the order parameter as an argument which determines the order of the spline interpolation. num_cols = self.df.select_dtypes(include=[np.number]).columns self.interpolated_df = self.df.copy() self.interpolated_df[num_cols] = self.interpolated_df[num_cols].interpolate(method='spline',order= self.order, limit_direction='both') It then clips the interpolated values to be non-negative and less than or equal to the maximum value before interpolation for each column. self.interpolated_df[num_cols] = self.interpolated_df[num_cols].clip(lower=0) for col in num_cols: max_val_before = self.df[col].max() self.interpolated_df[col] = self.interpolated_df[col].clip(upper=max_val_before) The result of the spline interpolation is stored in a new data frame called interpolated_df Plot Results The plot_results method takes in two arguments: Name Type Description Range/values Default Working_Column str The column/attribute of the input data frame which the user wants to plot List of the name of all the columns of the input data frame Appliances graph_limit int Number of samples of data on the x axis in line plot 1 to max size of rows/data samples in DataFrame 1000 The method plots the interpolated data against the original data using matplotlib. The interpolated data is plotted as a red line, while the original data is plotted as a blue line. The number of data points to be plotted is specified by the graph_limit argument. fig1, ax1 = plt.subplots(figsize=(15, 5)) ax1.plot(self.interpolated_df.index[:graph_limit], self.interpolated_df[Working_Column][:graph_limit], label='Interpolated Data',color='r', alpha=0.8) ax1.plot(self.df.index[:graph_limit], self.df[Working_Column][:graph_limit], label='Original Data',color='b', alpha=0.8) ax1.grid() ax1.legend() ax1.set_xlabel('Samples') ax1.set_ylabel('Value ') ax1.set_title(f\"Line Plot of Data Interpolation for {Working_Column}\") self.lineplot=fig1","title":"Interpolation"},{"location":"Data%20Manipulation/Interpolation/#interpolation","text":"The Interpolation class contains methods that perform different types of interpolation on time-series data. The purpose of this class is to fill missing values or gaps in the time series data, making it easier to analyze using four different techniques: forward fill, linear interpolation, cubic interpolation, and spline interpolation. The interpolated data can then be plotted alongside the original data to visualize the results.","title":"Interpolation"},{"location":"Data%20Manipulation/Interpolation/#class-attributes","text":"Name Type Description Range/values Default df pandas.core.frame.DataFrame Input dataframe - - resample_time str Frequency of time series data 1. A string that represents a fixed frequency offset. Some of the available string aliases include: \u2022 'D': daily frequency \u2022 'W': weekly frequency \u2022 'M': month frequency \u2022 'Q': quarter frequency \u2022 'Y': year frequency 2. A string that represents a custom frequency offset. Some of the available offset strings include: \u2022 '2H': every 2 hours \u2022 '3T'/'3MIN': every 3 minutes \u2022 '5S': every 5 seconds \u2022 '1D10H': every day and 10 hours 10MIN date_column str The name of the date column of the input data frame - date order int Order for the Spline Interpolation 1 - 5 2","title":"Class Attributes"},{"location":"Data%20Manipulation/Interpolation/#class-methods","text":"","title":"Class Methods"},{"location":"Data%20Manipulation/Interpolation/#class-initialization","text":"This method initializes the class attributes. df is the DataFrame containing the time-series data. resample_time is the resampling frequency of the data. date_column is the name of the column containing the dates or timestamps. order is the order of the spline interpolation. The method also calls the set_index method to set the DataFrame index.","title":"Class Initialization"},{"location":"Data%20Manipulation/Interpolation/#indexing-and-resampling","text":"set_index(self) method sets the DataFrame index to the date_column . self.df[self.date_column] = pd.to_datetime(self.df[self.date_column]) self.df = self.df.set_index(self.df[self.date_column]) The DataFrame is then resampled at the specified frequency in the resample_time . self.df = self.df.resample(self.resample_time).mean().reset_index() It also fills any missing values at the beginning and end of the DataFrame with the first and last valid values respectively. for column in self.df.select_dtypes(include=[np.number]): self.df[column].iloc[0] = self.df[column].iloc[self.df[column].first_valid_index()] for column in self.df.select_dtypes(include=[np.number]): self.df[column].iloc[-1] = self.df[column].fillna(method='ffill', limit=len(self.df)).iloc[-1]","title":"Indexing and Resampling"},{"location":"Data%20Manipulation/Interpolation/#forward-fill-method","text":"ffill(self) method fills missing values in the DataFrame using forward fill. This is achieved by creating a copy of the original DataFrame and assigning it to self.interpolated_df. Then, the ffill method is called on this new DataFrame, which fills any missing values with the last observed value before the missing value. This means that the missing value is filled with the most recently observed value. self.interpolated_df = self.df.copy() self.interpolated_df = self.interpolated_df.ffill() The result of the ffill interpolation is stored in a new data frame called interpolated_df .","title":"Forward Fill Method"},{"location":"Data%20Manipulation/Interpolation/#linear-interpolation","text":"linear(self) method performs linear interpolation on any missing values in the numerical columns of the Dataframe. This is achieved by creating a copy of the original DataFrame and assigning it to self.interpolated_df . Then, the interpolate method is called on the numerical columns of this new DataFrame with the method = linear and the limit_direction argument set to both . This method fills any missing values with a linearly interpolated value between the closest valid values in the column. num_cols = self.df.select_dtypes(include=[np.number]).columns self.interpolated_df = self.df.copy() self.interpolated_df[num_cols] = self.interpolated_df[num_cols].interpolate(method='linear', limit_direction='both') The result of the linear interpolation is stored in a new data frame called interpolated_df .","title":"Linear Interpolation"},{"location":"Data%20Manipulation/Interpolation/#cubic-interpolation","text":"cubic(self) method performs cubic interpolation on the numerical columns of the resampled Dataframe. It calls the interpolate method with method='cubic' and the limit_direction argument set to both . num_cols = self.df.select_dtypes(include=[np.number]).columns self.interpolated_df = self.df.copy() self.interpolated_df[num_cols] = self.interpolated_df[num_cols].interpolate(method='cubic', limit_direction='both') It then clips the values of the interpolated data. Any negative values in the interpolated_df DataFrame are replaced with 0 and any values that are greater than the maximum value of the working column before interpolation are also replaced with the maximum value using the same approach to remove outliers and keep data in range. self.interpolated_df[num_cols] = self.interpolated_df[num_cols].clip(lower=0) for col in num_cols: max_val_before = self.df[col].max() self.interpolated_df[col] = self.interpolated_df[col].clip(upper=max_val_before) The result of the cubic interpolation is stored in a new data frame called interpolated_df .","title":"Cubic Interpolation"},{"location":"Data%20Manipulation/Interpolation/#spline-interpolation","text":"spline(self) method performs interpolation using the spline method on the numerical columns of the data frame. It takes the order parameter as an argument which determines the order of the spline interpolation. num_cols = self.df.select_dtypes(include=[np.number]).columns self.interpolated_df = self.df.copy() self.interpolated_df[num_cols] = self.interpolated_df[num_cols].interpolate(method='spline',order= self.order, limit_direction='both') It then clips the interpolated values to be non-negative and less than or equal to the maximum value before interpolation for each column. self.interpolated_df[num_cols] = self.interpolated_df[num_cols].clip(lower=0) for col in num_cols: max_val_before = self.df[col].max() self.interpolated_df[col] = self.interpolated_df[col].clip(upper=max_val_before) The result of the spline interpolation is stored in a new data frame called interpolated_df","title":"Spline Interpolation"},{"location":"Data%20Manipulation/Interpolation/#plot-results","text":"The plot_results method takes in two arguments: Name Type Description Range/values Default Working_Column str The column/attribute of the input data frame which the user wants to plot List of the name of all the columns of the input data frame Appliances graph_limit int Number of samples of data on the x axis in line plot 1 to max size of rows/data samples in DataFrame 1000 The method plots the interpolated data against the original data using matplotlib. The interpolated data is plotted as a red line, while the original data is plotted as a blue line. The number of data points to be plotted is specified by the graph_limit argument. fig1, ax1 = plt.subplots(figsize=(15, 5)) ax1.plot(self.interpolated_df.index[:graph_limit], self.interpolated_df[Working_Column][:graph_limit], label='Interpolated Data',color='r', alpha=0.8) ax1.plot(self.df.index[:graph_limit], self.df[Working_Column][:graph_limit], label='Original Data',color='b', alpha=0.8) ax1.grid() ax1.legend() ax1.set_xlabel('Samples') ax1.set_ylabel('Value ') ax1.set_title(f\"Line Plot of Data Interpolation for {Working_Column}\") self.lineplot=fig1","title":"Plot Results"},{"location":"Data%20Manipulation/outliers/","text":"Outlier Recognition The goal of smoothing is to remove high-frequency noise and highlight important features of the signal, such as trends, patterns, and anomalies. It can be performed using various types of filters and it is used in a wide range of applications like: Time series analysis, Image processing, Signal processing, Data analysis. In each of these applications, smoothing can improve the accuracy of analyses and predictions, and provide a clearer view of important features in the data. Basic Libraries The below lines of the code imports required libraries to perform the outlier recognition such as NumPy, Pandas, SciPy, Matplotlib, and IsolationForest from scikit-learn. NumPy - NumPy is a popular library for numerical computing in Python. Pandas - Pandas is a popular library for data manipulation and analysis in Python. SciPy - The SciPy library is another popular library for scientific computing in Python. Matplotlib - Matplotlib is a popular library for creating visualizations in Python. IsolationForest - Isolation Forest is an unsupervised algorithm used for outlier detection. import numpy as np import pandas as pd import scipy.stats as stats import matplotlib.pyplot as plt from sklearn.ensemble import IsolationForest Class The purpose of this class is to perform outlier detection on the input data. This is class called \"Outliers_Recognization\" that takes two parameters in its constructor method: \"df\" and \"date_column\". These 2 parameters can be used throughout the class. This class can also be further extended to include different types of outlier detection algorithms, allowing for greater flexibility in analysis. class Outliers_Recognization(): def __init__(self, df, date_column): self.df = df self.date_column = date_column df -- is a pandas DataFrame that represents the data we want to analyze for outliers. date_column -- is the name of the column in the Data Frame that contains the date or time data that we want to use for our analysis. Z-Score The Z-Score method is based on the concept of standard deviation. In this method, an outlier is identified as a data point that falls outside a certain number of standard deviations from the mean of the dataset. Paramaters The method is to perform outlier detection on the numerical columns of a Pandas Data Frame using the Z-Score method. The method creates a new Data Frame with the same structure as the original Data Frame, but with any data points that exceed the specified threshold are marked as outliers and replaced with NaN. The method takes two arguments, self and threshold. def Z_score(self, threshold): Self refers to the instance of the class and is passed automatically when the method is called. Threshold is a user-defined value that will be used to determine whether a data point is an outlier or not. This value can be set as per the user. Explanation This line calculates the z-scores of the numerical columns of the DataFrame. First, self.df.select_dtypes(include=[np.number]) selects all columns with numerical data types (e.g. float, int) from the DataFrame. z = np.abs(stats.zscore(self.df[self.df.select_dtypes(include=[np.number]).columns])) Then, stats.zscore from the SciPy library is used to calculate the z-scores of these selected columns. The np.abs function is used to take the absolute value of the z-scores to ensure that all of the scores are positive. This line creates a new DataFrame called self.df_Without_Outliers by masking the original DataFrame with the condition (z > threshold). This means that any data point in the original DataFrame where the z-score is greater than the threshold will be replaced with NaN (i.e. masked). self.df_Without_Outliers = self.df.mask((z > threshold)) This line copies the values from the original DataFrame's date_column to the new self.df_Without_Outliers DataFrame, so that the dates are preserved in the resulting DataFrame. self.df_Without_Outliers[self.date_column] = self.df[self.date_column] Inter Quantile Range The interquartile range (IQR) is a measure of statistical dispersion that is often used in outlier recognition. The IQR is defined as the difference between the first quartile (Q1) and the third quartile (Q3) of a dataset, representing the range of values that contains the middle 50% of the data. This method is known as the Tukey method and is widely used in various fields such as finance, economics, and healthcare. It is important to note that while the IQR can help identify outliers, it should not be the only criterion for outlier recognition Paramaters This function is designed to identify outliers in a given pandas DataFrame using the Quantile method. The function calculates the lower and upper boundaries for each numeric column in the DataFrame using the specified quantile values and then removes any rows containing outliers outside these boundaries. The method takes three arguments, self, Q1 and Q2. def Quantile(self, Q1, Q2): self refers to the instance of the class and is passed automatically when the method is called. Q1 This is a numeric value between 0 and 1 that specifies the lower quantile value for identifying outliers. Any value less than or equal to this quantile will be considered an outlier and removed from the DataFrame. Q2 This is a numeric value between 0 and 1 that specifies the upper quantile value for identifying outliers. Any value greater than or equal to this quantile will be considered an outlier and removed from the DataFrame. Explanation The lower and upper boundaries for each numeric column in the DataFrame is found using the quantile() method of pandas. This step ensures that only numeric columns are considered for outlier detection. L_Q = self.df.select_dtypes(include=[np.number]).quantile(Q1, interpolation=\"nearest\") U_Q = self.df.select_dtypes(include=[np.number]).quantile(Q2, interpolation=\"nearest\") A copy of the original DataFrame without any outliers is created, using the mask() method of pandas and the lower and upper boundaries calculated in the previous step. self.df_Without_Outliers = self.df.mask((self.df.select_dtypes(include=[np.number]) > U_Q) | (self.df.select_dtypes(include=[np.number]) < L_Q)) To assign the original date column values to the new DataFrame without outliers, to preserve the temporal information. self.df_Without_Outliers[self.date_column]=self.df[self.date_column] Modified Z-Score The Modified Z-Score (MZS) is a method for identifying outliers in a dataset based on the median and the median absolute deviation (MAD) of the data. The MZS is a robust alternative to the traditional Z-Score method, which is sensitive to extreme values and outliers. Parameters The function calculates the Modified Z-score for each numeric column in the DataFrame and then masks any rows containing outliers above the specified threshold. The method takes two arguments, self and threshold. def Modified_Z_score(self,threshold): Self refers to the instance of the class and is passed automatically when the method is called. Threshold is a user-defined value that will be used to determine whether a data point is an outlier or not. This value can be set as per the user. Explanation To calculate the median of each numeric column in the DataFrame using the select_dtypes() and median() methods of pandas. This step ensures that only numeric columns are considered for outlier detection. median = self.df.select_dtypes(include=[np.number]).median() This calculates the Median Absolute Deviation (MAD) of each numeric column using the median_abs_deviation() function from the scipy.stats library. MAD = stats.median_abs_deviation(self.df.select_dtypes(include=[np.number])) Calculation of the Modified Z-score for each row and column in the DataFrame using the formula and the calculated median and MAD. z = 0.6745 * np.abs((self.df[self.df.select_dtypes(include=[np.number]).columns] - median) / MAD) The below line of code is used to create a copy of the original DataFrame without any outliers, using the mask() method and any() method of pandas.Modified Z-score greater than or equal to the threshold will be considered an outlier and removed from the DataFrame. self.df_Without_Outliers = self.df.mask((z >= threshold).any(axis=1)) This line copies the values from the original DataFrame's date_column to the new self.df_Without_Outliers DataFrame, so that the dates are preserved in the resulting DataFrame. self.df_Without_Outliers[self.date_column]=self.df[self.date_column] Isolation Forest In the Isolation Forest algorithm, a binary tree is constructed by randomly selecting a feature and a split point for the data at each node. The process is repeated recursively until each data point is isolated in its own leaf node. The path length from the root node to each leaf node is then used as a measure of outlierness, with shorter paths indicating that a data point is more likely to be an outlier. Paramaters The function creates an instance of the IsolationForest class from the scikit-learn library and fits it to the numerical data in the DataFrame. The model then predicts the outliers based on a given contamination rate and removes them from the DataFrame. The method takes two argument, self, contamination. def Isolation_Forest(self,contamination): self refers to the instance of the class and is passed automatically when the method is called. contamination This parameter specifies the proportion of outliers expected in the data. It should be a float value between 0 and 1, where higher values indicate a higher proportion of expected outliers. Explanation The numerical data is extracted from the DataFrame using the select_dtypes() method of pandas and convert it to a numpy array using the values attribute. This is done because the Isolation Forest algorithm can only be applied to numerical data. X = self.df.select_dtypes(include=[np.number]).values An instance of the IsolationForest class with the specified contamination rate is created and fit it to the numerical data using the fit() method. clf = IsolationForest(random_state=0, contamination=contamination) clf.fit(X) The outliers is predicted using the predict() method and identify them as the rows where the predicted value is -1. outliers = clf.predict(X) == -1 A copy of the original DataFrame is created and the values in the numerical columns of the rows identified as outliers are replaced with NaN values, using the loc[] method of pandas. self.df_Without_Outliers = self.df.copy() self.df_Without_Outliers.loc[outliers, self.df.select_dtypes(include=[np.number]).columns] = np.nan The original date column values are assigned to the new DataFrame without outliers, to preserve the temporal information. self.df_Without_Outliers[self.date_column]=self.df[self.date_column]","title":"Outliers"},{"location":"Data%20Manipulation/outliers/#outlier-recognition","text":"The goal of smoothing is to remove high-frequency noise and highlight important features of the signal, such as trends, patterns, and anomalies. It can be performed using various types of filters and it is used in a wide range of applications like: Time series analysis, Image processing, Signal processing, Data analysis. In each of these applications, smoothing can improve the accuracy of analyses and predictions, and provide a clearer view of important features in the data.","title":"Outlier Recognition"},{"location":"Data%20Manipulation/outliers/#basic-libraries","text":"The below lines of the code imports required libraries to perform the outlier recognition such as NumPy, Pandas, SciPy, Matplotlib, and IsolationForest from scikit-learn. NumPy - NumPy is a popular library for numerical computing in Python. Pandas - Pandas is a popular library for data manipulation and analysis in Python. SciPy - The SciPy library is another popular library for scientific computing in Python. Matplotlib - Matplotlib is a popular library for creating visualizations in Python. IsolationForest - Isolation Forest is an unsupervised algorithm used for outlier detection. import numpy as np import pandas as pd import scipy.stats as stats import matplotlib.pyplot as plt from sklearn.ensemble import IsolationForest","title":"Basic Libraries"},{"location":"Data%20Manipulation/outliers/#class","text":"The purpose of this class is to perform outlier detection on the input data. This is class called \"Outliers_Recognization\" that takes two parameters in its constructor method: \"df\" and \"date_column\". These 2 parameters can be used throughout the class. This class can also be further extended to include different types of outlier detection algorithms, allowing for greater flexibility in analysis. class Outliers_Recognization(): def __init__(self, df, date_column): self.df = df self.date_column = date_column df -- is a pandas DataFrame that represents the data we want to analyze for outliers. date_column -- is the name of the column in the Data Frame that contains the date or time data that we want to use for our analysis.","title":"Class"},{"location":"Data%20Manipulation/outliers/#z-score","text":"The Z-Score method is based on the concept of standard deviation. In this method, an outlier is identified as a data point that falls outside a certain number of standard deviations from the mean of the dataset.","title":"Z-Score"},{"location":"Data%20Manipulation/outliers/#paramaters","text":"The method is to perform outlier detection on the numerical columns of a Pandas Data Frame using the Z-Score method. The method creates a new Data Frame with the same structure as the original Data Frame, but with any data points that exceed the specified threshold are marked as outliers and replaced with NaN. The method takes two arguments, self and threshold. def Z_score(self, threshold): Self refers to the instance of the class and is passed automatically when the method is called. Threshold is a user-defined value that will be used to determine whether a data point is an outlier or not. This value can be set as per the user.","title":"Paramaters"},{"location":"Data%20Manipulation/outliers/#explanation","text":"This line calculates the z-scores of the numerical columns of the DataFrame. First, self.df.select_dtypes(include=[np.number]) selects all columns with numerical data types (e.g. float, int) from the DataFrame. z = np.abs(stats.zscore(self.df[self.df.select_dtypes(include=[np.number]).columns])) Then, stats.zscore from the SciPy library is used to calculate the z-scores of these selected columns. The np.abs function is used to take the absolute value of the z-scores to ensure that all of the scores are positive. This line creates a new DataFrame called self.df_Without_Outliers by masking the original DataFrame with the condition (z > threshold). This means that any data point in the original DataFrame where the z-score is greater than the threshold will be replaced with NaN (i.e. masked). self.df_Without_Outliers = self.df.mask((z > threshold)) This line copies the values from the original DataFrame's date_column to the new self.df_Without_Outliers DataFrame, so that the dates are preserved in the resulting DataFrame. self.df_Without_Outliers[self.date_column] = self.df[self.date_column]","title":"Explanation"},{"location":"Data%20Manipulation/outliers/#inter-quantile-range","text":"The interquartile range (IQR) is a measure of statistical dispersion that is often used in outlier recognition. The IQR is defined as the difference between the first quartile (Q1) and the third quartile (Q3) of a dataset, representing the range of values that contains the middle 50% of the data. This method is known as the Tukey method and is widely used in various fields such as finance, economics, and healthcare. It is important to note that while the IQR can help identify outliers, it should not be the only criterion for outlier recognition","title":"Inter Quantile Range"},{"location":"Data%20Manipulation/outliers/#paramaters_1","text":"This function is designed to identify outliers in a given pandas DataFrame using the Quantile method. The function calculates the lower and upper boundaries for each numeric column in the DataFrame using the specified quantile values and then removes any rows containing outliers outside these boundaries. The method takes three arguments, self, Q1 and Q2. def Quantile(self, Q1, Q2): self refers to the instance of the class and is passed automatically when the method is called. Q1 This is a numeric value between 0 and 1 that specifies the lower quantile value for identifying outliers. Any value less than or equal to this quantile will be considered an outlier and removed from the DataFrame. Q2 This is a numeric value between 0 and 1 that specifies the upper quantile value for identifying outliers. Any value greater than or equal to this quantile will be considered an outlier and removed from the DataFrame.","title":"Paramaters"},{"location":"Data%20Manipulation/outliers/#explanation_1","text":"The lower and upper boundaries for each numeric column in the DataFrame is found using the quantile() method of pandas. This step ensures that only numeric columns are considered for outlier detection. L_Q = self.df.select_dtypes(include=[np.number]).quantile(Q1, interpolation=\"nearest\") U_Q = self.df.select_dtypes(include=[np.number]).quantile(Q2, interpolation=\"nearest\") A copy of the original DataFrame without any outliers is created, using the mask() method of pandas and the lower and upper boundaries calculated in the previous step. self.df_Without_Outliers = self.df.mask((self.df.select_dtypes(include=[np.number]) > U_Q) | (self.df.select_dtypes(include=[np.number]) < L_Q)) To assign the original date column values to the new DataFrame without outliers, to preserve the temporal information. self.df_Without_Outliers[self.date_column]=self.df[self.date_column]","title":"Explanation"},{"location":"Data%20Manipulation/outliers/#modified-z-score","text":"The Modified Z-Score (MZS) is a method for identifying outliers in a dataset based on the median and the median absolute deviation (MAD) of the data. The MZS is a robust alternative to the traditional Z-Score method, which is sensitive to extreme values and outliers.","title":"Modified Z-Score"},{"location":"Data%20Manipulation/outliers/#parameters","text":"The function calculates the Modified Z-score for each numeric column in the DataFrame and then masks any rows containing outliers above the specified threshold. The method takes two arguments, self and threshold. def Modified_Z_score(self,threshold): Self refers to the instance of the class and is passed automatically when the method is called. Threshold is a user-defined value that will be used to determine whether a data point is an outlier or not. This value can be set as per the user.","title":"Parameters"},{"location":"Data%20Manipulation/outliers/#explanation_2","text":"To calculate the median of each numeric column in the DataFrame using the select_dtypes() and median() methods of pandas. This step ensures that only numeric columns are considered for outlier detection. median = self.df.select_dtypes(include=[np.number]).median() This calculates the Median Absolute Deviation (MAD) of each numeric column using the median_abs_deviation() function from the scipy.stats library. MAD = stats.median_abs_deviation(self.df.select_dtypes(include=[np.number])) Calculation of the Modified Z-score for each row and column in the DataFrame using the formula and the calculated median and MAD. z = 0.6745 * np.abs((self.df[self.df.select_dtypes(include=[np.number]).columns] - median) / MAD) The below line of code is used to create a copy of the original DataFrame without any outliers, using the mask() method and any() method of pandas.Modified Z-score greater than or equal to the threshold will be considered an outlier and removed from the DataFrame. self.df_Without_Outliers = self.df.mask((z >= threshold).any(axis=1)) This line copies the values from the original DataFrame's date_column to the new self.df_Without_Outliers DataFrame, so that the dates are preserved in the resulting DataFrame. self.df_Without_Outliers[self.date_column]=self.df[self.date_column]","title":"Explanation"},{"location":"Data%20Manipulation/outliers/#isolation-forest","text":"In the Isolation Forest algorithm, a binary tree is constructed by randomly selecting a feature and a split point for the data at each node. The process is repeated recursively until each data point is isolated in its own leaf node. The path length from the root node to each leaf node is then used as a measure of outlierness, with shorter paths indicating that a data point is more likely to be an outlier.","title":"Isolation Forest"},{"location":"Data%20Manipulation/outliers/#paramaters_2","text":"The function creates an instance of the IsolationForest class from the scikit-learn library and fits it to the numerical data in the DataFrame. The model then predicts the outliers based on a given contamination rate and removes them from the DataFrame. The method takes two argument, self, contamination. def Isolation_Forest(self,contamination): self refers to the instance of the class and is passed automatically when the method is called. contamination This parameter specifies the proportion of outliers expected in the data. It should be a float value between 0 and 1, where higher values indicate a higher proportion of expected outliers.","title":"Paramaters"},{"location":"Data%20Manipulation/outliers/#explanation_3","text":"The numerical data is extracted from the DataFrame using the select_dtypes() method of pandas and convert it to a numpy array using the values attribute. This is done because the Isolation Forest algorithm can only be applied to numerical data. X = self.df.select_dtypes(include=[np.number]).values An instance of the IsolationForest class with the specified contamination rate is created and fit it to the numerical data using the fit() method. clf = IsolationForest(random_state=0, contamination=contamination) clf.fit(X) The outliers is predicted using the predict() method and identify them as the rows where the predicted value is -1. outliers = clf.predict(X) == -1 A copy of the original DataFrame is created and the values in the numerical columns of the rows identified as outliers are replaced with NaN values, using the loc[] method of pandas. self.df_Without_Outliers = self.df.copy() self.df_Without_Outliers.loc[outliers, self.df.select_dtypes(include=[np.number]).columns] = np.nan The original date column values are assigned to the new DataFrame without outliers, to preserve the temporal information. self.df_Without_Outliers[self.date_column]=self.df[self.date_column]","title":"Explanation"},{"location":"Data%20Manipulation/smoothing/","text":"Smoothing Data The goal of smoothing is to remove high-frequency noise and highlight important features of the signal, such as trends, patterns, and anomalies. Class attributes Name Type Range/values Description data pandas.core.frame.DataFrame - Dataframe to work with. x_axis String - Select the column that will work as X axis y_axis String - Select the column that will work as Y axis(Only for plotting) x_start String 2 - 15 Initial point of the working range of X. (Only for plotting) x_end String - Final point of the working range of X. (Only for plotting) window_size Int 2 - 15 Refers to the number of data points used to calculate the smoothed value for a given point. grade int - Refers to the order of the polynomial that is fit to the data within the window alpha float 2 - 15 Is the smoothing factor that controls the weight given to past values in the calculation of a smoothed value. The indexes of the start and end of the range are obtained to define the working range: self.index_min = int(self.data.index[ self.data[self.x] == self.x_start].to_list()[0]) self.index_max = int(self.data.index[ self.data[self.x] == self.x_end].to_list()[0]) After that, a new array is created with indexes reset from 0 to N, the algorithms and the plots will be performed on this new array: self.new_x_axis = self.data[self.x][self.index_min: self.index_max] self.new_y_axis = self.data[self.y][self.index_min: self.index_max] self.new_x_axis = self.new_x_axis.reset_index(drop = True) self.new_y_axis = self.new_y_axis.reset_index(drop = True) self.y_filtered = [] self.y_filt_complete = [] Moving Average Filter The moving average filter works by taking the average of a set of data points over a certain window size, and using this average as the estimate of the signal value at any given point. It is used for signal processing, finance, and engineering, where the goal is to remove high-frequency noise and obtain a clearer representation of the underlying signal. Parameters Name Type Range/values Description self.new_y_axis String - Declaration of the Y axis (Data that will be analyzed) self.window_size Int 2 - 15 Refers to the number of data points used to calculate the smoothed value for a given point. Description of the method The method is defined and the array that will display the smoothed values is initialized to 0. def mov_average_filter(self): i = 0 moving_averages = [] A loop will iterate the number of times of the lenght of the data that will be smoothed, minus the size of the window. e.g., 150 rows - 10 as window size = 145 iterations. while i < len(self.new_y_axis) - self.window : On each iteration, starting from position 0, the average of the following N numbers will be calculated and the result will be appended to the new smoothed array. window_sum = self.new_y_axis[i : i + self.window] window_average = round(sum(window_sum) / self.window, 2) moving_averages.append(window_average) i += 1 As a time offset is generated when working with a moving average filter, due to the calculation of the current average based on future/past samples. A compensation before and after the smoothed array is implemented. for i in range (0, round(self.window/2), 1): moving_averages.insert(0,None) for i in range (0, round(self.window/2), 1): moving_averages.append(None) The final smoothed values are returned. y_filtered = moving_averages return y_filtered Savitzky-Golay Filter The Savitzky-Golay smoothing filter works by fitting a polynomial of a certain order to a set of data points and using this polynomial to estimate the value of the signal at any given point.It is useful in situations where it is important to preserve features such as peaks and valleys in the data. Parameters Name Type Range/values Description self.new_y_axis String - Declaration of the Y axis (Data that will be analyzed) self.window_size int 2 - 15 Refers to the number of data points used to calculate the smoothed value for a given point. self.poly_degree int 1 - 5 Refers to the order of the polynomial that is fit to the data within the window Description of the method The method is defined and calculated with the function 'savgol_filter' provided by the scipy.signal library. def savgol_filter(self): y_filtered = savgol_filter(self.new_y_axis, self.window, self.grade) return y_filtered Exponential Smoothing Filter Exponential smoothing works by weighting the past data points in a signal exponentially, with more recent data points receiving higher weight than older data points. The smoothed signal value at any given point is a weighted average of the past data points, with the weights decaying exponentially over time. It is particularly useful when the signal has trends or seasonality, as it can effectively capture these patterns. Parameters Name Type Range/values Description self.new_y_axis String - Declaration of the Y axis (Data that will be analyzed) self.alpha float 0 - 1 Is the smoothing factor that controls the weight given to past values in the calculation of a smoothed value. Description of the method The method is defined and the filtered array is initialized to 0. def exponential_filter(self): self.y_filtered = [self.new_y_axis[0]] A loop iterates from 0 to the lenght of the original data. for i in range(1, len(self.new_y_axis)): On each iteration, the new smoothed value is calculated from the following calculation. The result is appended to the smoothed array. smoothed_val = self.alpha * self.new_y_axis[i] + (1 - self.alpha) * self.y_filtered[i-1] self.y_filtered.append(smoothed_val) Create New DataFrame Generate a new Dataframe with the smoothed values obtained by the method selected Parameters Name Type Range/values Description method_name String - Provide the method selected by the user. df_to_change dataframe - Assign a new dataframe that will be filled with the filtered values. Description of the method After adjusting the parameters acording with the desired output, a for loop is performed to run the filtering algorithm in every column of the dataframe. The result will be stored in the new dataframe called \"df_to_change\" df = self.data column_to_skip = self.x if method_name == 'method_name': for column in df.columns: if column != column_to_skip: #The filtering method will be performed here... df_to_change[column] = filtered_column","title":"Smoothing Data"},{"location":"Data%20Manipulation/smoothing/#smoothing-data","text":"The goal of smoothing is to remove high-frequency noise and highlight important features of the signal, such as trends, patterns, and anomalies. Class attributes Name Type Range/values Description data pandas.core.frame.DataFrame - Dataframe to work with. x_axis String - Select the column that will work as X axis y_axis String - Select the column that will work as Y axis(Only for plotting) x_start String 2 - 15 Initial point of the working range of X. (Only for plotting) x_end String - Final point of the working range of X. (Only for plotting) window_size Int 2 - 15 Refers to the number of data points used to calculate the smoothed value for a given point. grade int - Refers to the order of the polynomial that is fit to the data within the window alpha float 2 - 15 Is the smoothing factor that controls the weight given to past values in the calculation of a smoothed value. The indexes of the start and end of the range are obtained to define the working range: self.index_min = int(self.data.index[ self.data[self.x] == self.x_start].to_list()[0]) self.index_max = int(self.data.index[ self.data[self.x] == self.x_end].to_list()[0]) After that, a new array is created with indexes reset from 0 to N, the algorithms and the plots will be performed on this new array: self.new_x_axis = self.data[self.x][self.index_min: self.index_max] self.new_y_axis = self.data[self.y][self.index_min: self.index_max] self.new_x_axis = self.new_x_axis.reset_index(drop = True) self.new_y_axis = self.new_y_axis.reset_index(drop = True) self.y_filtered = [] self.y_filt_complete = []","title":"Smoothing Data"},{"location":"Data%20Manipulation/smoothing/#moving-average-filter","text":"The moving average filter works by taking the average of a set of data points over a certain window size, and using this average as the estimate of the signal value at any given point. It is used for signal processing, finance, and engineering, where the goal is to remove high-frequency noise and obtain a clearer representation of the underlying signal.","title":"Moving Average Filter"},{"location":"Data%20Manipulation/smoothing/#parameters","text":"Name Type Range/values Description self.new_y_axis String - Declaration of the Y axis (Data that will be analyzed) self.window_size Int 2 - 15 Refers to the number of data points used to calculate the smoothed value for a given point.","title":"Parameters"},{"location":"Data%20Manipulation/smoothing/#description-of-the-method","text":"The method is defined and the array that will display the smoothed values is initialized to 0. def mov_average_filter(self): i = 0 moving_averages = [] A loop will iterate the number of times of the lenght of the data that will be smoothed, minus the size of the window. e.g., 150 rows - 10 as window size = 145 iterations. while i < len(self.new_y_axis) - self.window : On each iteration, starting from position 0, the average of the following N numbers will be calculated and the result will be appended to the new smoothed array. window_sum = self.new_y_axis[i : i + self.window] window_average = round(sum(window_sum) / self.window, 2) moving_averages.append(window_average) i += 1 As a time offset is generated when working with a moving average filter, due to the calculation of the current average based on future/past samples. A compensation before and after the smoothed array is implemented. for i in range (0, round(self.window/2), 1): moving_averages.insert(0,None) for i in range (0, round(self.window/2), 1): moving_averages.append(None) The final smoothed values are returned. y_filtered = moving_averages return y_filtered","title":"Description of the method"},{"location":"Data%20Manipulation/smoothing/#savitzky-golay-filter","text":"The Savitzky-Golay smoothing filter works by fitting a polynomial of a certain order to a set of data points and using this polynomial to estimate the value of the signal at any given point.It is useful in situations where it is important to preserve features such as peaks and valleys in the data.","title":"Savitzky-Golay Filter"},{"location":"Data%20Manipulation/smoothing/#parameters_1","text":"Name Type Range/values Description self.new_y_axis String - Declaration of the Y axis (Data that will be analyzed) self.window_size int 2 - 15 Refers to the number of data points used to calculate the smoothed value for a given point. self.poly_degree int 1 - 5 Refers to the order of the polynomial that is fit to the data within the window","title":"Parameters"},{"location":"Data%20Manipulation/smoothing/#description-of-the-method_1","text":"The method is defined and calculated with the function 'savgol_filter' provided by the scipy.signal library. def savgol_filter(self): y_filtered = savgol_filter(self.new_y_axis, self.window, self.grade) return y_filtered","title":"Description of the method"},{"location":"Data%20Manipulation/smoothing/#exponential-smoothing-filter","text":"Exponential smoothing works by weighting the past data points in a signal exponentially, with more recent data points receiving higher weight than older data points. The smoothed signal value at any given point is a weighted average of the past data points, with the weights decaying exponentially over time. It is particularly useful when the signal has trends or seasonality, as it can effectively capture these patterns.","title":"Exponential Smoothing Filter"},{"location":"Data%20Manipulation/smoothing/#parameters_2","text":"Name Type Range/values Description self.new_y_axis String - Declaration of the Y axis (Data that will be analyzed) self.alpha float 0 - 1 Is the smoothing factor that controls the weight given to past values in the calculation of a smoothed value.","title":"Parameters"},{"location":"Data%20Manipulation/smoothing/#description-of-the-method_2","text":"The method is defined and the filtered array is initialized to 0. def exponential_filter(self): self.y_filtered = [self.new_y_axis[0]] A loop iterates from 0 to the lenght of the original data. for i in range(1, len(self.new_y_axis)): On each iteration, the new smoothed value is calculated from the following calculation. The result is appended to the smoothed array. smoothed_val = self.alpha * self.new_y_axis[i] + (1 - self.alpha) * self.y_filtered[i-1] self.y_filtered.append(smoothed_val)","title":"Description of the method"},{"location":"Data%20Manipulation/smoothing/#create-new-dataframe","text":"Generate a new Dataframe with the smoothed values obtained by the method selected","title":"Create New DataFrame"},{"location":"Data%20Manipulation/smoothing/#parameters_3","text":"Name Type Range/values Description method_name String - Provide the method selected by the user. df_to_change dataframe - Assign a new dataframe that will be filled with the filtered values.","title":"Parameters"},{"location":"Data%20Manipulation/smoothing/#description-of-the-method_3","text":"After adjusting the parameters acording with the desired output, a for loop is performed to run the filtering algorithm in every column of the dataframe. The result will be stored in the new dataframe called \"df_to_change\" df = self.data column_to_skip = self.x if method_name == 'method_name': for column in df.columns: if column != column_to_skip: #The filtering method will be performed here... df_to_change[column] = filtered_column","title":"Description of the method"},{"location":"User%20Interface/GUI/","text":"Graphical User Interface using Streamlit The Data Science and Visualization Application was developed using the library Streamlit , which is completely open-source and based on python. The entirely open-source, Python-based library Streamlit was used in the creation of the Data Science and Visualization Application. This makes Streamlit very approachable and relatively simple to understand and use. The program is set up as a series of separate pages, each of which serves a distinct function. The program uses a variety of concepts, including AI, machine learning, and data processing. The project repository contains more details on the application and the associated source code. Why Streamlit ? Open source and free to use. Completely based on python , and hence no prior experience with front-end development is needed. Simple, yet strong API with vast number of supportive elements for creating interactive user interfaces . Support for Github-flavored Markdown , LaTeX expressions and HTML tags. Adding widgets ( st.button , st.checkbox , etc.) to gather user inputs is as simple as declaring variables. Functionality to cache and reload data objects in the current user session. Support for various charting libraries ( matplotlib.pyplot , Bokeh , Altair , Plotly , etc.). Easy installation , setup and running of the developed applications. Structure of the Application : Developed as a collection of multiple individual pages , which can exchange information amongst each other. Creation of (parent) classes from all groups describing the parameters required to run the developed class methods. Possible use of Data classes (or Child classes ) for gathering relevant user inputs (datasets, parameters, etc.) and exchanging information amongst the (parent) classes. Each page describes certain functionality within the application and works primarily with a certain group of (parent and data) classes . On each page, Objects of relevant classes are created and initialized with gathered user-inputs, and relevant class methods are called to generate corresponding textual or visual outputs . The computed results are stored back as class variables , and are then displayed on the user interface . Some of the results are also stored in session cache for further use in subsequent application pages. GUI_Class : Primary class used to store basic information about the selected/uploaded dataset. ( Source Code ) Parameters : arg_df : selected dataframe arg_filename : corresponding file name (if present, example \"divorce.csv\" )","title":"Graphical User Interface using Streamlit"},{"location":"User%20Interface/GUI/#graphical-user-interface-using-streamlit","text":"The Data Science and Visualization Application was developed using the library Streamlit , which is completely open-source and based on python. The entirely open-source, Python-based library Streamlit was used in the creation of the Data Science and Visualization Application. This makes Streamlit very approachable and relatively simple to understand and use. The program is set up as a series of separate pages, each of which serves a distinct function. The program uses a variety of concepts, including AI, machine learning, and data processing. The project repository contains more details on the application and the associated source code.","title":"Graphical User Interface using Streamlit"},{"location":"User%20Interface/GUI/#why-streamlit","text":"Open source and free to use. Completely based on python , and hence no prior experience with front-end development is needed. Simple, yet strong API with vast number of supportive elements for creating interactive user interfaces . Support for Github-flavored Markdown , LaTeX expressions and HTML tags. Adding widgets ( st.button , st.checkbox , etc.) to gather user inputs is as simple as declaring variables. Functionality to cache and reload data objects in the current user session. Support for various charting libraries ( matplotlib.pyplot , Bokeh , Altair , Plotly , etc.). Easy installation , setup and running of the developed applications.","title":"Why Streamlit ?"},{"location":"User%20Interface/GUI/#structure-of-the-application","text":"Developed as a collection of multiple individual pages , which can exchange information amongst each other. Creation of (parent) classes from all groups describing the parameters required to run the developed class methods. Possible use of Data classes (or Child classes ) for gathering relevant user inputs (datasets, parameters, etc.) and exchanging information amongst the (parent) classes. Each page describes certain functionality within the application and works primarily with a certain group of (parent and data) classes . On each page, Objects of relevant classes are created and initialized with gathered user-inputs, and relevant class methods are called to generate corresponding textual or visual outputs . The computed results are stored back as class variables , and are then displayed on the user interface . Some of the results are also stored in session cache for further use in subsequent application pages.","title":"Structure of the Application :"},{"location":"User%20Interface/GUI/#gui_class","text":"Primary class used to store basic information about the selected/uploaded dataset. ( Source Code )","title":"GUI_Class :"},{"location":"User%20Interface/GUI/#parameters","text":"arg_df : selected dataframe arg_filename : corresponding file name (if present, example \"divorce.csv\" )","title":"Parameters :"}]}